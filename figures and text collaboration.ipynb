{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32baa749-ec27-44af-ba95-0feffe43dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Build an article-level master dataset by aggregating image features\n",
    "and merging with the text/topics table.\n",
    "\n",
    "Inputs:\n",
    "  - articles_topics_bestk.xlsx         # text-level (one row per article)\n",
    "  - Final_images_data.xlsx             # image-level (one row per image)\n",
    "    required columns on image side: article_url, Core Figure Gender, Core Figure Skin, role\n",
    "Outputs:\n",
    "  - merged_row_level.xlsx              # row-level text×image merge (multi-rows per article if multi-images)\n",
    "  - article_image_agg.xlsx             # image features aggregated to article level (one row per article that has images)\n",
    "  - master_article_dataset.xlsx        # text table LEFT-JOIN with article-level image aggregates (one row per article)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- Paths ----------\n",
    "TEXT_XLSX = \"articles_topics_bestk.xlsx\"\n",
    "IMG_XLSX  = \"Final_images_data.xlsx\"\n",
    "\n",
    "OUT_MERGED_ROW  = \"merged_row_level.xlsx\"\n",
    "OUT_IMG_AGG     = \"article_image_agg.xlsx\"\n",
    "OUT_MASTER      = \"master_article_dataset.xlsx\"\n",
    "\n",
    "# ---------- Load ----------\n",
    "text_df = pd.read_excel(TEXT_XLSX)\n",
    "img_df  = pd.read_excel(IMG_XLSX)\n",
    "\n",
    "# ---------- Key harmonization ----------\n",
    "KEY = \"article_url\"\n",
    "# coerce to string and strip\n",
    "for df in (text_df, img_df):\n",
    "    if KEY not in df.columns:\n",
    "        raise ValueError(f\"Missing key column '{KEY}' in: {df.shape}\")\n",
    "    df[KEY] = df[KEY].astype(str).str.strip()\n",
    "\n",
    "# ---------- Basic diagnostics ----------\n",
    "print(\"---- KEY diagnostics ----\")\n",
    "print(\"text key dtype:\", text_df[KEY].dtype)\n",
    "print(\"img  key dtype:\", img_df[KEY].dtype)\n",
    "print(\"text key nunique:\", text_df[KEY].nunique())\n",
    "print(\"img  key nunique:\", img_df[KEY].nunique())\n",
    "inter = set(text_df[KEY]).intersection(set(img_df[KEY]))\n",
    "print(\"intersection size:\", len(inter))\n",
    "print(\"text-only keys   :\", text_df[~text_df[KEY].isin(img_df[KEY])][KEY].nunique())\n",
    "print(\"image-only keys  :\", img_df[~img_df[KEY].isin(text_df[KEY])][KEY].nunique())\n",
    "\n",
    "# ---------- Find/standardize image feature columns ----------\n",
    "# Try to robustly locate columns (case/space tolerant)\n",
    "def find_col(candidates, df):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols:\n",
    "            return cols[cand.lower()]\n",
    "    # fuzzy match\n",
    "    for c in df.columns:\n",
    "        if any(cand.lower() in c.lower() for cand in candidates):\n",
    "            return c\n",
    "    raise ValueError(f\"Could not find any of {candidates} in image dataframe columns: {list(df.columns)[:10]} ...\")\n",
    "\n",
    "COL_GENDER = find_col([\"Core Figure Gender\", \"core_figure_gender\", \"gender\"], img_df)\n",
    "COL_SKIN   = find_col([\"Core Figure Skin\", \"core_figure_skin\", \"skin\", \"skintone\"], img_df)\n",
    "COL_ROLE   = find_col([\"role\", \"core_role\", \"figure_role\"], img_df)\n",
    "\n",
    "print(f\"[Image columns] gender='{COL_GENDER}', skin='{COL_SKIN}', role='{COL_ROLE}'\")\n",
    "\n",
    "# ---------- Clean & normalize codes as string (so counting is consistent) ----------\n",
    "def norm_code(series):\n",
    "    s = series.copy()\n",
    "    # keep 99 or NA as '99' for \"unknown\"\n",
    "    s = s.where(s.notna(), 99)\n",
    "    # cast numerics to int safely; otherwise keep strings\n",
    "    try:\n",
    "        s = pd.to_numeric(s, errors=\"coerce\").fillna(99).astype(int).astype(str)\n",
    "    except Exception:\n",
    "        s = s.astype(str).str.strip().replace({\"\": \"99\"})\n",
    "    return s\n",
    "\n",
    "img_df[COL_GENDER] = norm_code(img_df[COL_GENDER])\n",
    "img_df[COL_SKIN]   = norm_code(img_df[COL_SKIN])\n",
    "img_df[COL_ROLE]   = norm_code(img_df[COL_ROLE])\n",
    "\n",
    "# ---------- Merge to get row-level joined table (optional but useful for audits) ----------\n",
    "merged = text_df.merge(img_df[[KEY, COL_GENDER, COL_SKIN, COL_ROLE]], on=KEY, how=\"left\")\n",
    "print(f\"Merged rows: {len(merged)} | articles: {text_df[KEY].nunique()}\")\n",
    "merged.to_excel(OUT_MERGED_ROW, index=False)\n",
    "print(f\"[OK] Saved row-level merge -> {OUT_MERGED_ROW}\")\n",
    "\n",
    "# ---------- Article-level aggregation of image features ----------\n",
    "def dist_cols(prefix, counter, valid_keys=None):\n",
    "    \"\"\"Convert a Counter into wide columns with share and count.\n",
    "       If valid_keys provided, only keep those codes explicitly (others go to 'other').\"\"\"\n",
    "    total = sum(counter.values())\n",
    "    out = {}\n",
    "    if total == 0:\n",
    "        return out\n",
    "    keys = valid_keys or sorted(counter.keys())\n",
    "    for k in keys:\n",
    "        cnt = counter.get(k, 0)\n",
    "        out[f\"{prefix}{k}_cnt\"]   = cnt\n",
    "        out[f\"{prefix}{k}_share\"] = cnt / total\n",
    "    # optional: collect unknown/other\n",
    "    if valid_keys is not None:\n",
    "        other_cnt = sum(v for kk, v in counter.items() if kk not in valid_keys)\n",
    "        out[f\"{prefix}other_cnt\"]   = other_cnt\n",
    "        out[f\"{prefix}other_share\"] = other_cnt / total\n",
    "    return out\n",
    "\n",
    "def aggregate_article(group):\n",
    "    n_imgs = len(group)\n",
    "    # distributions\n",
    "    g_cnt = Counter(group[COL_GENDER])\n",
    "    s_cnt = Counter(group[COL_SKIN])\n",
    "    r_cnt = Counter(group[COL_ROLE])\n",
    "\n",
    "    # majority labels (mode)\n",
    "    maj_gender = max(g_cnt, key=g_cnt.get) if g_cnt else None\n",
    "    maj_skin   = max(s_cnt, key=s_cnt.get) if s_cnt else None\n",
    "    maj_role   = max(r_cnt, key=r_cnt.get) if r_cnt else None\n",
    "\n",
    "    # shares dictionaries (limit to known codes to keep columns stable)\n",
    "    # You can adjust the known code lists if your coding scheme differs.\n",
    "    gender_out = dist_cols(\"gender_\", g_cnt, valid_keys=[\"1\",\"2\",\"3\",\"99\"])\n",
    "    skin_out   = dist_cols(\"skin_\"  , s_cnt, valid_keys=[\"1\",\"2\",\"3\",\"4\",\"99\"])\n",
    "    role_out   = dist_cols(\"role_\"  , r_cnt, valid_keys=[\"1\",\"2\",\"3\",\"4\",\"5\",\"99\"])\n",
    "\n",
    "    # diversity flags\n",
    "    skin_diverse = int(len([k for k in s_cnt if k not in (\"99\",)]) >= 2)  # ≥2 skin codes (excluding unknown)\n",
    "    gender_mixed = int(len([k for k in g_cnt if k not in (\"99\",)]) >= 2)\n",
    "\n",
    "    out = {\n",
    "        KEY: group.iloc[0][KEY],\n",
    "        \"n_images\": n_imgs,\n",
    "        \"maj_gender\": maj_gender,\n",
    "        \"maj_skin\": maj_skin,\n",
    "        \"maj_role\": maj_role,\n",
    "        \"skin_diverse_flag\": skin_diverse,\n",
    "        \"gender_mixed_flag\": gender_mixed,\n",
    "    }\n",
    "    out.update(gender_out)\n",
    "    out.update(skin_out)\n",
    "    out.update(role_out)\n",
    "    return pd.Series(out)\n",
    "\n",
    "img_agg = merged.dropna(subset=[COL_GENDER, COL_SKIN, COL_ROLE], how=\"all\") \\\n",
    "                .groupby(KEY, as_index=False).apply(aggregate_article)\n",
    "\n",
    "img_agg.to_excel(OUT_IMG_AGG, index=False)\n",
    "print(f\"[OK] Saved image article aggregates -> {OUT_IMG_AGG}\")\n",
    "\n",
    "# ---------- Build master dataset (LEFT JOIN text with image aggregates) ----------\n",
    "master = text_df.merge(img_agg, on=KEY, how=\"left\")\n",
    "master.to_excel(OUT_MASTER, index=False)\n",
    "print(f\"[OK] Saved master article dataset -> {OUT_MASTER}\")\n",
    "\n",
    "# ---------- Quick sanity prints ----------\n",
    "print(\"\\n[Master head]\")\n",
    "print(master[[KEY, \"topic_label\", \"n_images\", \"maj_gender\", \"maj_skin\", \"maj_role\"]].head())\n",
    "\n",
    "print(\"\\n[Null image aggregates by topic (top 10)]\")\n",
    "print(master[master[\"n_images\"].isna()].groupby(\"topic_label\").size().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1f1da-1205-45de-865c-cf1d1c3a1b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Config: file paths ----------------\n",
    "IMG_XLSX     = \"final_images_data1.xlsx\"       # image-level table\n",
    "ARTICLE_XLSX = \"stance_dual_by_year.xlsx\"      # text-level table (sheet=article_level)\n",
    "\n",
    "OUT_DIR   = \"outputs_align_text_image\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_IMG_CLEAN   = os.path.join(OUT_DIR, \"images_clean_labeled.xlsx\")\n",
    "OUT_IMG_AGG     = os.path.join(OUT_DIR, \"article_image_agg.xlsx\")\n",
    "OUT_MASTER      = os.path.join(OUT_DIR, \"master_article_merged.xlsx\")\n",
    "OUT_PIVOTS_XLSX = os.path.join(OUT_DIR, \"topic_image_alignment_pivots.xlsx\")\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def find_col(cols, patterns, required=True):\n",
    "    \"\"\"\n",
    "    Fuzzy column finder: returns the first column whose lowercase name matches any regex pattern.\n",
    "    \"\"\"\n",
    "    cl = [c for c in cols]\n",
    "    lc = [c.lower() for c in cols]\n",
    "    for pat in patterns:\n",
    "        for i, name in enumerate(lc):\n",
    "            if re.search(pat, name):\n",
    "                return cl[i]\n",
    "    if required:\n",
    "        raise KeyError(f\"Cannot find any column by patterns: {patterns}\")\n",
    "    return None\n",
    "\n",
    "# ---------------- Read ----------------\n",
    "img = pd.read_excel(IMG_XLSX)\n",
    "txt = pd.read_excel(ARTICLE_XLSX, sheet_name=\"article_level\")\n",
    "\n",
    "# ---------------- Normalize join key ----------------\n",
    "img_url_col = find_col(img.columns, [r\"^article[_ ]?url$\"])\n",
    "txt_url_col = find_col(txt.columns, [r\"^article[_ ]?url$\"])\n",
    "img[img_url_col] = img[img_url_col].astype(str).str.strip()\n",
    "txt[txt_url_col] = txt[txt_url_col].astype(str).str.strip()\n",
    "\n",
    "# ---------------- Locate image feature columns ----------------\n",
    "col_age    = find_col(img.columns, [r\"core.*age\"])\n",
    "col_gender = find_col(img.columns, [r\"core.*gend\"])\n",
    "col_skin   = find_col(img.columns, [r\"core.*skin\"])\n",
    "col_role   = find_col(img.columns, [r\"^role$\"])\n",
    "\n",
    "# ---------------- Mappings ----------------\n",
    "age_map = {1:\"Child\", 2:\"Adult\", 3:\"Elderly\", 99:\"Uncertain\"}\n",
    "gender_map = {1:\"Male\", 2:\"Female\", 3:\"Unclear\", 99:\"Unclear\"}\n",
    "skin_map = {1:\"White\", 2:\"Black\", 3:\"Beige\", 4:\"Mixed/Unclear\", 99:\"Unclear\"}\n",
    "role_map = {1:\"Elite\", 2:\"General public\", 99:\"Unclear/Abstract\"}\n",
    "\n",
    "# ---------------- Clean & map ----------------\n",
    "def to_int_safe(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        return 99\n",
    "\n",
    "img[col_age]    = img[col_age].apply(to_int_safe).map(age_map).fillna(\"Unclear\")\n",
    "img[col_gender] = img[col_gender].apply(to_int_safe).map(gender_map).fillna(\"Unclear\")\n",
    "img[col_skin]   = img[col_skin].apply(to_int_safe).map(skin_map).fillna(\"Unclear\")\n",
    "img[col_role]   = img[col_role].apply(to_int_safe).map(role_map).fillna(\"Unclear/Abstract\")\n",
    "\n",
    "img.to_excel(OUT_IMG_CLEAN, index=False)\n",
    "\n",
    "# ---------------- Aggregate image features ----------------\n",
    "def agg_share(df, key, cat_col, prefix):\n",
    "    ct = (df.groupby([key, cat_col]).size().rename(\"count\").reset_index())\n",
    "    tot = ct.groupby(key)[\"count\"].sum().rename(\"total\").reset_index()\n",
    "    ct = ct.merge(tot, on=key, how=\"left\")\n",
    "    ct[\"share\"] = ct[\"count\"] / ct[\"total\"]\n",
    "    w_count = ct.pivot(index=key, columns=cat_col, values=\"count\").fillna(0)\n",
    "    w_share = ct.pivot(index=key, columns=cat_col, values=\"share\").fillna(0.0)\n",
    "    w_count.columns = [f\"{prefix}_{c}_count\" for c in w_count.columns]\n",
    "    w_share.columns = [f\"{prefix}_{c}_share\" for c in w_share.columns]\n",
    "    wide = pd.concat([w_count, w_share], axis=1).reset_index()\n",
    "    return wide\n",
    "\n",
    "key = img_url_col\n",
    "agg_gender = agg_share(img, key, col_gender, \"gender\")\n",
    "agg_skin   = agg_share(img, key, col_skin,   \"skin\")\n",
    "agg_role   = agg_share(img, key, col_role,   \"role\")\n",
    "agg_age    = agg_share(img, key, col_age,    \"age\")\n",
    "\n",
    "def dominant_label(df_wide, prefix):\n",
    "    cnt_cols = [c for c in df_wide.columns if c.startswith(prefix) and c.endswith(\"_count\")]\n",
    "    lab = df_wide[cnt_cols].idxmax(axis=1).str.replace(f\"{prefix}_\", \"\", regex=False).str.replace(\"_count\",\"\", regex=False)\n",
    "    return lab.rename(f\"{prefix}_dominant\")\n",
    "\n",
    "for wide in [agg_gender, agg_skin, agg_role, agg_age]:\n",
    "    prefix = wide.columns[1].split(\"_\")[0]\n",
    "    wide[prefix + \"_dominant\"] = dominant_label(wide, prefix)\n",
    "\n",
    "img_agg = agg_gender.merge(agg_skin, on=key, how=\"outer\") \\\n",
    "                    .merge(agg_role, on=key, how=\"outer\") \\\n",
    "                    .merge(agg_age,   on=key, how=\"outer\")\n",
    "img_agg.to_excel(OUT_IMG_AGG, index=False)\n",
    "\n",
    "# ---------------- Merge with text ----------------\n",
    "topic_col = find_col(txt.columns, [r\"topic[_ ]?label\"], required=False)\n",
    "sent_col  = find_col(txt.columns, [r\"sent[_ ]?mean\"], required=False)\n",
    "title_col = find_col(txt.columns, [r\"^title$\"], required=False)\n",
    "\n",
    "text_keep = [c for c in [txt_url_col, title_col, topic_col, sent_col] if c]\n",
    "master = txt[text_keep].drop_duplicates()\n",
    "master = master.merge(img_agg, left_on=txt_url_col, right_on=img_url_col, how=\"left\")\n",
    "master.to_excel(OUT_MASTER, index=False)\n",
    "\n",
    "print(\"Done. Outputs saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632999e3-f7b6-4059-b838-c0348dcab837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MASTER_XLSX = \"outputs_align_text_image/master_article_merged.xlsx\"\n",
    "\n",
    "# ---------------- Load ----------------\n",
    "df = pd.read_excel(MASTER_XLSX)\n",
    "\n",
    "# Choose the topic column\n",
    "topic_col = \"topic_label\"   # change if your text table uses another name\n",
    "\n",
    "# ---------------- Visualization: stacked bar ----------------\n",
    "def plot_stacked_bar(df, topic_col, prefix, top_n=15):\n",
    "    \"\"\"\n",
    "    Make stacked bar chart: topic × category share\n",
    "    \"\"\"\n",
    "    share_cols = [c for c in df.columns if c.startswith(prefix) and c.endswith(\"_share\")]\n",
    "    if not share_cols:\n",
    "        print(f\"No share cols found for prefix={prefix}\")\n",
    "        return\n",
    "    \n",
    "    # average share by topic\n",
    "    mat = df.groupby(topic_col)[share_cols].mean()\n",
    "    mat = mat[mat.sum(axis=1) > 0]   # drop empty topics\n",
    "    \n",
    "    # keep only top_n topics (by article count)\n",
    "    topic_counts = df[topic_col].value_counts().head(top_n).index\n",
    "    mat = mat.loc[mat.index.intersection(topic_counts)]\n",
    "    \n",
    "    # plot\n",
    "    mat.plot(kind=\"bar\", stacked=True, figsize=(12,6))\n",
    "    plt.title(f\"{prefix.capitalize()} distribution across topics\")\n",
    "    plt.ylabel(\"Average share\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------- Visualization: heatmap ----------------\n",
    "def plot_heatmap(df, topic_col, prefix, top_n=20):\n",
    "    \"\"\"\n",
    "    Heatmap of topic × dominant label counts\n",
    "    \"\"\"\n",
    "    dom_col = f\"{prefix}_dominant\"\n",
    "    if dom_col not in df.columns:\n",
    "        print(f\"No dominant column found for {prefix}\")\n",
    "        return\n",
    "    \n",
    "    ct = pd.crosstab(df[topic_col], df[dom_col], normalize=\"index\")\n",
    "    \n",
    "    # keep only top_n topics\n",
    "    top_topics = df[topic_col].value_counts().head(top_n).index\n",
    "    ct = ct.loc[ct.index.intersection(top_topics)]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.imshow(ct, aspect=\"auto\", cmap=\"Blues\")\n",
    "    plt.colorbar(label=\"Share\")\n",
    "    plt.xticks(range(len(ct.columns)), ct.columns, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(ct.index)), ct.index)\n",
    "    plt.title(f\"Heatmap of {prefix} dominant labels across topics\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "for prefix in [\"gender\", \"skin\", \"role\"]:\n",
    "    plot_stacked_bar(df, topic_col, prefix)\n",
    "    plot_heatmap(df, topic_col, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0e865-33a8-49d6-84d7-72865a4348e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ========= Paths =========\n",
    "TXT_XLSX = \"stance_dual_by_year.xlsx\"   # Must include sheet \"article_level\"\n",
    "IMG_XLSX = \"final_images_data1.xlsx\"    # Must include article_url + role\n",
    "OUT_DIR  = \"multimodal_article_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ========= Helpers =========\n",
    "def role_code_to_label(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    if isinstance(x, (int,float)):\n",
    "        x = int(x)\n",
    "        return {1:\"Elite\",2:\"General Public\",99:\"Unclear/Abstract\"}.get(x,\"Unclear/Abstract\")\n",
    "    s = str(x).lower()\n",
    "    if \"elite\" in s: return \"Elite\"\n",
    "    if \"general\" in s or \"public\" in s: return \"General Public\"\n",
    "    if \"abstract\" in s or \"unclear\" in s: return \"Unclear/Abstract\"\n",
    "    return \"Unclear/Abstract\"\n",
    "\n",
    "def normalize_stance(s):\n",
    "    s = str(s).strip().title()\n",
    "    return {\"Pos\":\"Positive\",\"Neg\":\"Negative\",\"Neu\":\"Neutral\"}.get(s, s)\n",
    "\n",
    "def majority(series):\n",
    "    s = series.dropna()\n",
    "    if s.empty: return np.nan\n",
    "    return s.value_counts().idxmax()\n",
    "\n",
    "def crosstab_and_plots(df, title_prefix, out_prefix):\n",
    "    ct = pd.crosstab(df[\"stance_dual\"], df[\"img_role\"]).fillna(0).astype(int)\n",
    "    ct.to_csv(os.path.join(OUT_DIR, f\"{out_prefix}_crosstab_counts.csv\"))\n",
    "\n",
    "    # Heatmap\n",
    "    mat = ct.values\n",
    "    plt.figure(figsize=(6,4.6))\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(mat, aspect=\"auto\")\n",
    "    ax.set_xticks(range(ct.shape[1])); ax.set_xticklabels(ct.columns, rotation=25, ha=\"right\")\n",
    "    ax.set_yticks(range(ct.shape[0])); ax.set_yticklabels(ct.index)\n",
    "    for i in range(ct.shape[0]):\n",
    "        for j in range(ct.shape[1]):\n",
    "            ax.text(j, i, str(ct.iloc[i, j]), ha=\"center\", va=\"center\")\n",
    "    plt.title(f\"{title_prefix} — Crosstab (Counts)\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{out_prefix}_counts_heatmap.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    # Row-percent stacked bar\n",
    "    pct = ct.div(ct.sum(axis=1).replace(0,np.nan), axis=0).fillna(0)\n",
    "    x = np.arange(len(pct.index))\n",
    "    bottom = np.zeros(len(pct.index))\n",
    "    plt.figure(figsize=(7.2,4.6))\n",
    "    for col in pct.columns:\n",
    "        plt.bar(x, pct[col].values, bottom=bottom, label=col)\n",
    "        bottom += pct[col].values\n",
    "    plt.xticks(x, pct.index, rotation=25, ha=\"right\")\n",
    "    plt.ylabel(\"Share\")\n",
    "    plt.title(f\"{title_prefix} — Crosstab (Row %)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{out_prefix}_rowpct_stacked.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "# ========= 1) Read TEXT (article_level sheet) =========\n",
    "txt = pd.read_excel(TXT_XLSX, sheet_name=\"article_level\")\n",
    "txt[\"article_url\"] = txt[\"article_url\"].astype(str).str.strip()\n",
    "txt[\"stance_dual\"] = txt[\"stance_dual\"].apply(normalize_stance)\n",
    "txt[\"Year\"] = pd.to_numeric(txt[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ========= 2) Read IMAGES =========\n",
    "img = pd.read_excel(IMG_XLSX, sheet_name=0)\n",
    "img[\"article_url\"] = img[\"article_url\"].astype(str).str.strip()\n",
    "img[\"img_role\"] = img[\"role\"].apply(role_code_to_label)\n",
    "\n",
    "# ========= 3) Collapse images to article-level =========\n",
    "img_article = img.groupby(\"article_url\")[\"img_role\"].agg(majority).reset_index()\n",
    "\n",
    "# ========= 4) Merge TEXT + IMAGES =========\n",
    "df_article = txt.merge(img_article, on=\"article_url\", how=\"inner\")\n",
    "\n",
    "# Save article-level joined table\n",
    "df_article.to_csv(os.path.join(OUT_DIR, \"article_level_join.csv\"), index=False)\n",
    "\n",
    "# ========= 5) Global Crosstab =========\n",
    "crosstab_and_plots(df_article, \"Article-level (All)\", \"article_overall\")\n",
    "\n",
    "# ========= 6) By big_category (if available) =========\n",
    "if \"big_category\" in df_article.columns:\n",
    "    for bc, sub in df_article.groupby(\"big_category\"):\n",
    "        if sub.empty: continue\n",
    "        safe = str(bc).replace(\"/\",\"-\").replace(\" \",\"_\")\n",
    "        crosstab_and_plots(sub, f\"Article-level — {bc}\", f\"article_bycat_{safe}\")\n",
    "\n",
    "print(\"Done. Outputs saved to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238ea37-065d-4285-a87a-c3de2b124e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# ========== Paths ==========\n",
    "TXT_XLSX = \"stance_dual_by_year.xlsx\"   # must include sheet: \"article_level\"\n",
    "IMG_XLSX = \"final_images_data1.xlsx\"    # must include: article_url + identity columns\n",
    "OUT_DIR  = \"multimodal_article_outputs_strict\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SIG_THR = 2.0  # threshold for standardized residual significance\n",
    "\n",
    "# ========== Helpers ==========\n",
    "def normalize_stance(s):\n",
    "    s = str(s).strip().title()\n",
    "    return {\"Pos\": \"Positive\", \"Neg\": \"Negative\", \"Neu\": \"Neutral\"}.get(s, s)\n",
    "\n",
    "def _to_int_if_str_digit(x):\n",
    "    try:\n",
    "        if isinstance(x, str) and x.strip().isdigit():\n",
    "            return int(x.strip())\n",
    "    except Exception:\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "def role_code_to_label(x):\n",
    "    \"\"\"\n",
    "    Role coding: 1=Elite; 2=General Public; 99=Unclear/Abstract\n",
    "    \"\"\"\n",
    "    x = _to_int_if_str_digit(x)\n",
    "    if pd.isna(x):\n",
    "        return \"Unclear/Abstract\"\n",
    "    if isinstance(x, (int, float)) and np.isfinite(x):\n",
    "        return {1: \"Elite\", 2: \"General Public\", 99: \"Unclear/Abstract\"}.get(int(x), \"Unclear/Abstract\")\n",
    "    s = str(x).strip().lower()\n",
    "    if \"elite\" in s: return \"Elite\"\n",
    "    if \"general\" in s or \"public\" in s: return \"General Public\"\n",
    "    return \"Unclear/Abstract\"\n",
    "\n",
    "def gender_code_to_label(x):\n",
    "    \"\"\"\n",
    "    Core Figure Gender: 1=Male; 2=Female; 3=Unclear\n",
    "    \"\"\"\n",
    "    x = _to_int_if_str_digit(x)\n",
    "    if pd.isna(x):\n",
    "        return \"Unclear\"\n",
    "    if isinstance(x, (int, float)) and np.isfinite(x):\n",
    "        return {1: \"Male\", 2: \"Female\", 3: \"Unclear\"}.get(int(x), \"Unclear\")\n",
    "    s = str(x).strip().lower()\n",
    "    if \"male\" in s or \"man\" in s: return \"Male\"\n",
    "    if \"female\" in s or \"woman\" in s: return \"Female\"\n",
    "    return \"Unclear\"\n",
    "\n",
    "def skin_code_to_label(x):\n",
    "    \"\"\"\n",
    "    Core Figure skin: 1=White; 2=Black; 3=Beige (also 'Begie'); 4=Mixed/Unclear\n",
    "    \"\"\"\n",
    "    x = _to_int_if_str_digit(x)\n",
    "    if pd.isna(x):\n",
    "        return \"Unclear\"\n",
    "    if isinstance(x, (int, float)) and np.isfinite(x):\n",
    "        return {1: \"White\", 2: \"Black\", 3: \"Beige\", 4: \"Mixed/Unclear\"}.get(int(x), \"Unclear\")\n",
    "    s = str(x).strip().lower()\n",
    "    if \"white\" in s: return \"White\"\n",
    "    if \"black\" in s: return \"Black\"\n",
    "    if \"beige\" in s or \"begie\" in s: return \"Beige\"  # fix common misspelling\n",
    "    if \"mixed\" in s: return \"Mixed/Unclear\"\n",
    "    return \"Unclear\"\n",
    "\n",
    "def normalize_url(u, drop_query=True, drop_fragment=True):\n",
    "    \"\"\"Standardize URL for joining.\"\"\"\n",
    "    if pd.isna(u):\n",
    "        return np.nan\n",
    "    u = str(u).strip()\n",
    "    if not u:\n",
    "        return np.nan\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        scheme = (p.scheme or \"http\").lower()\n",
    "        netloc = p.netloc.lower()\n",
    "        path = p.path.rstrip(\"/\")\n",
    "        query = \"\" if drop_query else p.query\n",
    "        fragment = \"\" if drop_fragment else p.fragment\n",
    "        return urlunparse((scheme, netloc, path, p.params, query, fragment))\n",
    "    except Exception:\n",
    "        return u.strip().lower().rstrip(\"/\")\n",
    "\n",
    "def majority(series):\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return np.nan\n",
    "    return s.value_counts().idxmax()\n",
    "\n",
    "def find_col(df, candidates):\n",
    "    \"\"\"Case-insensitive exact match; return original column name or None.\"\"\"\n",
    "    cand_norm = [str(x).strip().lower() for x in candidates]\n",
    "    for col in df.columns:\n",
    "        if str(col).strip().lower() in cand_norm:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# ========== Chi-square and residuals ==========\n",
    "def chi2_with_std_resid(ct: pd.DataFrame):\n",
    "    \"\"\"Return chi2, p, dof, expected_df, std_resid_df, Cramer's V.\"\"\"\n",
    "    ct = ct.astype(float)\n",
    "    n = ct.values.sum()\n",
    "    if n == 0:\n",
    "        expected_df = pd.DataFrame(0, index=ct.index, columns=ct.columns)\n",
    "        std_resid = pd.DataFrame(np.nan, index=ct.index, columns=ct.columns)\n",
    "        return 0.0, 1.0, (ct.shape[0]-1)*(ct.shape[1]-1), expected_df, std_resid, 0.0\n",
    "\n",
    "    chi2, p, dof, expected = chi2_contingency(ct.values, correction=False)\n",
    "    expected_df = pd.DataFrame(expected, index=ct.index, columns=ct.columns)\n",
    "\n",
    "    row_p = ct.sum(axis=1) / n\n",
    "    col_p = ct.sum(axis=0) / n\n",
    "    denom = np.sqrt(expected_df * (1 - row_p.values[:, None]) * (1 - col_p.values[None, :]))\n",
    "    std_resid = (ct - expected_df) / denom.replace(0, np.nan)\n",
    "\n",
    "    r, c = ct.shape\n",
    "    cramers_v = np.sqrt(chi2 / (n * (min(r - 1, c - 1)))) if min(r - 1, c - 1) > 0 else 0.0\n",
    "    return chi2, p, dof, expected_df, std_resid, cramers_v\n",
    "\n",
    "def plot_crosstab_full(df, by_col, title_prefix, out_prefix, out_dir, sig_thr=2.0):\n",
    "    \"\"\"\n",
    "    Build Sentiment × by_col crosstab; export tables and three plots; export candidate URLs.\n",
    "    \"\"\"\n",
    "    # contingency\n",
    "    ct = pd.crosstab(df[\"stance_dual\"], df[by_col]).fillna(0).astype(int)\n",
    "    chi2, p, dof, expected, std_resid, cramers_v = chi2_with_std_resid(ct)\n",
    "    pct = ct.div(ct.sum(axis=1).replace(0, np.nan), axis=0).fillna(0)\n",
    "\n",
    "    # export tables\n",
    "    xlsx_path = os.path.join(out_dir, f\"{out_prefix}_crosstab.xlsx\")\n",
    "    with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as w:\n",
    "        ct.to_excel(w, \"counts\")\n",
    "        pct.to_excel(w, \"row_pct\")\n",
    "        expected.round(2).to_excel(w, \"expected\")\n",
    "        std_resid.round(3).to_excel(w, \"std_resid\")\n",
    "        (std_resid.abs() >= sig_thr).astype(int).to_excel(w, f\"sig_mask_abs>={sig_thr:g}\")\n",
    "\n",
    "    # counts heatmap\n",
    "    plt.figure(figsize=(6, 4.6))\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(ct.values, aspect=\"auto\")\n",
    "    ax.set_xticks(range(ct.shape[1])); ax.set_xticklabels(ct.columns, rotation=25, ha=\"right\")\n",
    "    ax.set_yticks(range(ct.shape[0])); ax.set_yticklabels(ct.index)\n",
    "    for i in range(ct.shape[0]):\n",
    "        for j in range(ct.shape[1]):\n",
    "            ax.text(j, i, str(ct.iloc[i, j]), ha=\"center\", va=\"center\")\n",
    "    plt.title(f\"{title_prefix} — Crosstab (Counts)\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, f\"{out_prefix}_counts_heatmap.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    # row-% stacked bars\n",
    "    x = np.arange(len(pct.index))\n",
    "    bottom = np.zeros(len(pct.index))\n",
    "    plt.figure(figsize=(7.2, 4.6))\n",
    "    for col in pct.columns:\n",
    "        plt.bar(x, pct[col].values, bottom=bottom, label=col)\n",
    "        bottom += pct[col].values\n",
    "    plt.xticks(x, pct.index, rotation=25, ha=\"right\")\n",
    "    plt.ylabel(\"Share\")\n",
    "    plt.title(f\"{title_prefix} — Crosstab (Row %)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, f\"{out_prefix}_rowpct_stacked.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    # standardized residuals heatmap with significance boxes\n",
    "    import matplotlib.patches as patches\n",
    "    vmax = float(np.nanmax(np.abs(std_resid.values))) if std_resid.size else 1.0\n",
    "    plt.figure(figsize=(6.6, 4.8))\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(std_resid.values, vmin=-vmax, vmax=vmax, aspect=\"auto\")\n",
    "    ax.set_xticks(range(std_resid.shape[1])); ax.set_xticklabels(std_resid.columns, rotation=25, ha=\"right\")\n",
    "    ax.set_yticks(range(std_resid.shape[0])); ax.set_yticklabels(std_resid.index)\n",
    "    for i in range(std_resid.shape[0]):\n",
    "        for j in range(std_resid.shape[1]):\n",
    "            val = std_resid.iloc[i, j]\n",
    "            ax.text(j, i, (\"\" if pd.isna(val) else f\"{val:.2f}\"), ha=\"center\", va=\"center\", fontsize=10)\n",
    "            if pd.notna(val) and abs(val) >= sig_thr:\n",
    "                ax.add_patch(patches.Rectangle((j - 0.5, i - 0.5), 1, 1, linewidth=2,\n",
    "                                               edgecolor=\"black\", facecolor=\"none\"))\n",
    "    plt.title(f\"{title_prefix} — Std. Residuals (chi2={chi2:.2f}, p={p:.2e}, V={cramers_v:.3f}; thr={sig_thr:g})\")\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"Standardized Residual\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, f\"{out_prefix}_std_resid_heatmap.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    # export candidate URLs (over-represented and under-represented cells)\n",
    "    res_map = std_resid.stack().to_dict()\n",
    "    df_tmp = df.loc[:, [\"article_url\", \"stance_dual\", by_col]].dropna()\n",
    "    df_tmp[\"std_resid\"] = df_tmp.apply(lambda r: res_map.get((r[\"stance_dual\"], r[by_col])), axis=1)\n",
    "    over  = df_tmp[df_tmp[\"std_resid\"] >=  sig_thr].copy()\n",
    "    under = df_tmp[df_tmp[\"std_resid\"] <= -sig_thr].copy()\n",
    "\n",
    "    cand_path = os.path.join(out_dir, f\"{out_prefix}_candidates.xlsx\")\n",
    "    with pd.ExcelWriter(cand_path, engine=\"openpyxl\") as w:\n",
    "        over.sort_values(\"std_resid\", ascending=False).to_excel(w, \"over_rep\", index=False)\n",
    "        under.sort_values(\"std_resid\", ascending=True).to_excel(w, \"under_rep\", index=False)\n",
    "\n",
    "    return {\"chi2\": chi2, \"p\": p, \"cramers_v\": cramers_v,\n",
    "            \"counts\": ct, \"row_pct\": pct, \"std_resid\": std_resid}\n",
    "\n",
    "# Unified export for role/gender/skin candidates (including tension for role)\n",
    "def export_unified_candidates(df_article, out_path, sig_thr=2.0):\n",
    "    writer = pd.ExcelWriter(out_path, engine=\"openpyxl\")\n",
    "\n",
    "    def residual_base(by_col):\n",
    "        ct = pd.crosstab(df_article[\"stance_dual\"], df_article[by_col]).fillna(0).astype(int)\n",
    "        if ct.empty or ct.sum().sum() == 0:\n",
    "            return None, None\n",
    "        _, _, _, _, std_resid, _ = chi2_with_std_resid(ct.astype(float))\n",
    "        mapping = std_resid.stack().to_dict()\n",
    "        base = df_article.loc[:, [\"article_url\", \"stance_dual\", by_col]].dropna()\n",
    "        base[\"std_resid\"] = base.apply(lambda r: mapping.get((r[\"stance_dual\"], r[by_col])), axis=1)\n",
    "        return base, std_resid\n",
    "\n",
    "    # Role: synergy, absence, tension\n",
    "    base, std_role = residual_base(\"img_role\")\n",
    "    if base is not None:\n",
    "        synergy_pairs = {(\"Positive\", \"Elite\"), (\"Negative\", \"General Public\")}\n",
    "        tension_pairs = {(\"Positive\", \"General Public\"), (\"Negative\", \"Elite\")}\n",
    "        role_synergy = base[(base[\"std_resid\"] >= sig_thr) &\n",
    "                            (base.apply(lambda r: (r[\"stance_dual\"], r[\"img_role\"]) in synergy_pairs, axis=1))].copy()\n",
    "        role_absence = base[(base[\"std_resid\"] <= -sig_thr)].copy()\n",
    "        role_tension = base[base.apply(lambda r: (r[\"stance_dual\"], r[\"img_role\"]) in tension_pairs, axis=1)].copy()\n",
    "        role_tension = role_tension.sort_values(\"std_resid\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "        role_synergy.sort_values(\"std_resid\", ascending=False).to_excel(writer, \"role_synergy\", index=False)\n",
    "        role_absence.sort_values(\"std_resid\", ascending=True).to_excel(writer, \"role_absence\", index=False)\n",
    "        role_tension.to_excel(writer, \"role_tension\", index=False)\n",
    "        if std_role is not None:\n",
    "            std_role.round(3).to_excel(writer, \"role_std_resid\")\n",
    "\n",
    "    # Gender: synergy (positive residual) and absence (negative residual)\n",
    "    if \"img_gender\" in df_article.columns:\n",
    "        base, std_gender = residual_base(\"img_gender\")\n",
    "        if base is not None:\n",
    "            g_syn = base[base[\"std_resid\"] >= sig_thr].copy()\n",
    "            g_abs = base[base[\"std_resid\"] <= -sig_thr].copy()\n",
    "            g_syn.sort_values(\"std_resid\", ascending=False).to_excel(writer, \"gender_synergy\", index=False)\n",
    "            g_abs.sort_values(\"std_resid\", ascending=True).to_excel(writer, \"gender_absence\", index=False)\n",
    "            if std_gender is not None:\n",
    "                std_gender.round(3).to_excel(writer, \"gender_std_resid\")\n",
    "\n",
    "    # Skin: synergy and absence\n",
    "    if \"img_skin\" in df_article.columns:\n",
    "        base, std_skin = residual_base(\"img_skin\")\n",
    "        if base is not None:\n",
    "            s_syn = base[base[\"std_resid\"] >= sig_thr].copy()\n",
    "            s_abs = base[base[\"std_resid\"] <= -sig_thr].copy()\n",
    "            s_syn.sort_values(\"std_resid\", ascending=False).to_excel(writer, \"skin_synergy\", index=False)\n",
    "            s_abs.sort_values(\"std_resid\", ascending=True).to_excel(writer, \"skin_absence\", index=False)\n",
    "            if std_skin is not None:\n",
    "                std_skin.round(3).to_excel(writer, \"skin_std_resid\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# ========== 1) Read and normalize text ==========\n",
    "txt = pd.read_excel(TXT_XLSX, sheet_name=\"article_level\")\n",
    "assert \"article_url\" in txt.columns, \"Text sheet 'article_level' must include article_url\"\n",
    "\n",
    "txt[\"article_url_raw\"] = txt[\"article_url\"]\n",
    "txt[\"article_url\"] = txt[\"article_url\"].apply(lambda x: normalize_url(x, drop_query=True, drop_fragment=True))\n",
    "txt = txt.dropna(subset=[\"article_url\"]).copy()\n",
    "\n",
    "if \"stance_dual\" not in txt.columns:\n",
    "    raise ValueError(\"Text sheet must include 'stance_dual'\")\n",
    "txt[\"stance_dual\"] = txt[\"stance_dual\"].apply(normalize_stance)\n",
    "\n",
    "if \"Year\" in txt.columns:\n",
    "    txt[\"Year\"] = pd.to_numeric(txt[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "else:\n",
    "    txt[\"Year\"] = pd.NA\n",
    "\n",
    "txt_agg = (\n",
    "    txt.groupby(\"article_url\")\n",
    "       .agg(\n",
    "           stance_dual=(\"stance_dual\", majority),\n",
    "           Year=(\"Year\", majority),\n",
    "           big_category=(\"big_category\", majority) if \"big_category\" in txt.columns else (\"Year\", lambda s: \"All\")\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "\n",
    "# ========== 2) Read and normalize image sheet ==========\n",
    "img = pd.read_excel(IMG_XLSX, sheet_name=0)\n",
    "assert \"article_url\" in img.columns, \"Image sheet must include article_url\"\n",
    "img.columns = [str(c).strip() for c in img.columns]  # trim header whitespace\n",
    "\n",
    "# detect columns (case-insensitive), including your exact names\n",
    "role_col   = find_col(img, [\"img_role\", \"role\", \"main_role\", \"Role\", \"Core Figure Role\"])\n",
    "gender_col = find_col(img, [\"Core Figure Gender\", \"img_gender\", \"gender\", \"Gender\",\n",
    "                            \"gender_dom\", \"gender_dominant\", \"main_gender\"])\n",
    "skin_col   = find_col(img, [\"Core Figure skin\", \"Core Figure Skin\", \"img_skin\", \"skin\", \"Skin\",\n",
    "                            \"skin_dom\", \"skin_dominant\", \"skin_tone\", \"skin_tone_dom\"])\n",
    "if role_col is None:\n",
    "    raise ValueError(\"Image sheet must include a role column (e.g., 'Core Figure Role' or 'role').\")\n",
    "\n",
    "img[\"article_url_raw\"] = img[\"article_url\"]\n",
    "img[\"article_url\"] = img[\"article_url\"].apply(lambda x: normalize_url(x, drop_query=True, drop_fragment=True))\n",
    "img = img.dropna(subset=[\"article_url\"]).copy()\n",
    "\n",
    "img[\"img_role\"] = img[role_col].apply(role_code_to_label)\n",
    "if gender_col is not None:\n",
    "    img[\"img_gender\"] = img[gender_col].apply(gender_code_to_label)\n",
    "if skin_col is not None:\n",
    "    img[\"img_skin\"] = img[skin_col].apply(skin_code_to_label)\n",
    "\n",
    "# aggregate to one row per article by majority vote\n",
    "agg_dict = {\"img_role\": (\"img_role\", majority)}\n",
    "if \"img_gender\" in img.columns:\n",
    "    agg_dict[\"img_gender\"] = (\"img_gender\", majority)\n",
    "if \"img_skin\" in img.columns:\n",
    "    agg_dict[\"img_skin\"] = (\"img_skin\", majority)\n",
    "\n",
    "img_agg = img.groupby(\"article_url\").agg(**agg_dict).reset_index()\n",
    "\n",
    "# quick check\n",
    "print(\"Detected columns -> role_col =\", role_col, \"| gender_col =\", gender_col, \"| skin_col =\", skin_col)\n",
    "\n",
    "# ========== 3) Intersect URLs and export unmatched lists ==========\n",
    "text_urls  = set(txt_agg[\"article_url\"].unique())\n",
    "image_urls = set(img_agg[\"article_url\"].unique())\n",
    "intersect  = text_urls & image_urls\n",
    "\n",
    "pd.DataFrame({\"text_only_urls\": sorted(text_urls - image_urls)}).to_excel(\n",
    "    os.path.join(OUT_DIR, \"unmatched_text_only_urls.xlsx\"), index=False\n",
    ")\n",
    "pd.DataFrame({\"image_only_urls\": sorted(image_urls - text_urls)}).to_excel(\n",
    "    os.path.join(OUT_DIR, \"unmatched_image_only_urls.xlsx\"), index=False\n",
    ")\n",
    "\n",
    "# keep intersection\n",
    "txt_keep = txt_agg[txt_agg[\"article_url\"].isin(intersect)].copy()\n",
    "img_keep = img_agg[img_agg[\"article_url\"].isin(intersect)].copy()\n",
    "\n",
    "# ========== 4) Strict inner join ==========\n",
    "df_article = txt_keep.merge(img_keep, on=\"article_url\", how=\"inner\")\n",
    "assert df_article[\"article_url\"].is_unique, \"Merged article_url should be unique per row\"\n",
    "\n",
    "# save joined table\n",
    "article_xlsx = os.path.join(OUT_DIR, \"article_level_join_strict.xlsx\")\n",
    "df_article.to_excel(article_xlsx, index=False)\n",
    "\n",
    "# ========== 5) Crosstabs and visualizations ==========\n",
    "stance_order = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "role_order   = [\"Elite\", \"General Public\", \"Unclear/Abstract\"]\n",
    "gender_order = [\"Male\", \"Female\", \"Unclear\"]\n",
    "skin_order   = [\"White\", \"Black\", \"Beige\", \"Mixed/Unclear\", \"Unclear\"]\n",
    "\n",
    "df_article[\"stance_dual\"] = pd.Categorical(df_article[\"stance_dual\"], categories=stance_order, ordered=False)\n",
    "df_article[\"img_role\"]    = pd.Categorical(df_article[\"img_role\"],    categories=role_order,   ordered=False)\n",
    "if \"img_gender\" in df_article.columns:\n",
    "    df_article[\"img_gender\"] = pd.Categorical(df_article[\"img_gender\"], categories=gender_order, ordered=False)\n",
    "if \"img_skin\" in df_article.columns:\n",
    "    df_article[\"img_skin\"]   = pd.Categorical(df_article[\"img_skin\"],   categories=skin_order,   ordered=False)\n",
    "\n",
    "# Overall: Sentiment × Role/Gender/Skin\n",
    "plot_crosstab_full(df_article, \"img_role\",\n",
    "                   \"Article-level (Strict) — Sentiment × Role\",\n",
    "                   \"article_overall_strict_role\", OUT_DIR, sig_thr=SIG_THR)\n",
    "\n",
    "if \"img_gender\" in df_article.columns:\n",
    "    plot_crosstab_full(df_article, \"img_gender\",\n",
    "                       \"Article-level (Strict) — Sentiment × Gender\",\n",
    "                       \"article_overall_strict_gender\", OUT_DIR, sig_thr=SIG_THR)\n",
    "\n",
    "if \"img_skin\" in df_article.columns:\n",
    "    plot_crosstab_full(df_article, \"img_skin\",\n",
    "                       \"Article-level (Strict) — Sentiment × Skin\",\n",
    "                       \"article_overall_strict_skin\", OUT_DIR, sig_thr=SIG_THR)\n",
    "\n",
    "# By big_category (if present)\n",
    "if \"big_category\" in df_article.columns:\n",
    "    for bc, sub in df_article.groupby(\"big_category\"):\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        safe = str(bc).replace(\"/\", \"-\").replace(\"\\\\\", \"-\").replace(\" \", \"_\")\n",
    "        plot_crosstab_full(sub, \"img_role\",\n",
    "                           f\"Article-level — {bc} (Strict) — S×Role\",\n",
    "                           f\"article_bycat_{safe}_strict_role\", OUT_DIR, sig_thr=SIG_THR)\n",
    "        if \"img_gender\" in sub.columns:\n",
    "            plot_crosstab_full(sub, \"img_gender\",\n",
    "                               f\"Article-level — {bc} (Strict) — S×Gender\",\n",
    "                               f\"article_bycat_{safe}_strict_gender\", OUT_DIR, sig_thr=SIG_THR)\n",
    "        if \"img_skin\" in sub.columns:\n",
    "            plot_crosstab_full(sub, \"img_skin\",\n",
    "                               f\"Article-level — {bc} (Strict) — S×Skin\",\n",
    "                               f\"article_bycat_{safe}_strict_skin\", OUT_DIR, sig_thr=SIG_THR)\n",
    "\n",
    "# ========== 6) Unified candidates export ==========\n",
    "export_unified_candidates(\n",
    "    df_article,\n",
    "    os.path.join(OUT_DIR, \"sentiment_identity_candidates_unified.xlsx\"),\n",
    "    sig_thr=SIG_THR\n",
    ")\n",
    "\n",
    "# ========== 7) Quick summary ==========\n",
    "with open(os.path.join(OUT_DIR, \"summary_stats.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Text raw rows: {len(txt)}\\n\")\n",
    "    f.write(f\"Text unique URLs: {txt_agg['article_url'].nunique()}\\n\")\n",
    "    f.write(f\"Image raw rows: {len(img)}\\n\")\n",
    "    f.write(f\"Image unique URLs: {img_agg['article_url'].nunique()}\\n\")\n",
    "    f.write(f\"Intersection URLs: {len(intersect)}\\n\")\n",
    "    f.write(f\"Final joined rows (strict 1:1): {len(df_article)}\\n\")\n",
    "\n",
    "print(\"Done. Outputs in:\", OUT_DIR)\n",
    "print(\"XLSX files written:\",\n",
    "      article_xlsx,\n",
    "      os.path.join(OUT_DIR, \"unmatched_text_only_urls.xlsx\"),\n",
    "      os.path.join(OUT_DIR, \"unmatched_image_only_urls.xlsx\"),\n",
    "      os.path.join(OUT_DIR, \"sentiment_identity_candidates_unified.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d1530-a1db-4a60-b6ff-13d9afd05352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
