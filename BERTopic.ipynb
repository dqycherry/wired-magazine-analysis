{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c432c-6d80-4295-bc76-3b79a5569904",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic sentence-transformers umap-learn hdbscan pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bab23a-980f-47a2-bfa5-cbc099db7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from bertopic import BERTopic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ================== 1) Load data ==================\n",
    "df = pd.read_excel(\"ai_articles_with_person_cleaned.xlsx\")\n",
    "# keep only articles that have at least one image (for later multimodal pairing)\n",
    "df = df[df[\"num_images\"] > 0].copy()\n",
    "texts = df[\"text_cleaned_final\"].dropna().astype(str).tolist()\n",
    "\n",
    "\n",
    "# ================== 2) Build sentence embeddings ==================\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-small\")\n",
    "\n",
    "embeddings = []\n",
    "batch_size = 32\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    emb = embedding_model.encode(batch, show_progress_bar=False, normalize_embeddings=False)\n",
    "    embeddings.extend(emb)\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "\n",
    "# ================== 3) Dimensionality reduction (UMAP) ==================\n",
    "UMAP_KW = dict(\n",
    "    n_neighbors=57,\n",
    "    n_components=5,\n",
    "    min_dist=0.0233,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    ")\n",
    "umap_model = UMAP(**UMAP_KW)\n",
    "embeddings_umap = umap_model.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "# ================== 4) NPMI computation ==================\n",
    "def compute_npmi(topics_words, tokenized_texts):\n",
    "    \"\"\"\n",
    "    Compute per-topic NPMI over the top words.\n",
    "    topics_words: list[list[str]]  -- words for each topic\n",
    "    tokenized_texts: list[list[str]] -- documents tokenized into unique tokens\n",
    "    \"\"\"\n",
    "    N = len(tokenized_texts)\n",
    "    df_word = Counter()\n",
    "    df_pair = Counter()\n",
    "\n",
    "    # document-frequency counts (single and pair)\n",
    "    for doc in tokenized_texts:\n",
    "        uniq = set(doc)\n",
    "        for w in uniq:\n",
    "            df_word[w] += 1\n",
    "        for w1, w2 in combinations(sorted(uniq), 2):\n",
    "            df_pair[(w1, w2)] += 1\n",
    "\n",
    "    def npmi(w1, w2):\n",
    "        key = (w1, w2) if w1 <= w2 else (w2, w1)\n",
    "        c12 = df_pair.get(key, 0)\n",
    "        if c12 == 0:\n",
    "            return None\n",
    "        p1 = df_word.get(w1, 0) / N\n",
    "        p2 = df_word.get(w2, 0) / N\n",
    "        p12 = c12 / N\n",
    "        # PMI with tiny eps for numerical stability, then normalise\n",
    "        pmi = math.log((p12 + 1e-12) / (p1 * p2 + 1e-12))\n",
    "        return pmi / (-math.log(p12 + 1e-12))\n",
    "\n",
    "    def topic_npmi(words):\n",
    "        vals = []\n",
    "        for a, b in combinations(words, 2):\n",
    "            v = npmi(a, b)\n",
    "            if v is not None:\n",
    "                vals.append(v)\n",
    "        return float(np.mean(vals)) if vals else float(\"nan\")\n",
    "\n",
    "    return [topic_npmi(ws) for ws in topics_words]\n",
    "\n",
    "\n",
    "# ================== 5) Scan candidate k; pick the best ==================\n",
    "candidate_k = [10, 15, 20, 25, 30]\n",
    "results = []\n",
    "\n",
    "for k in candidate_k:\n",
    "    # Step 1: KMeans clustering in UMAP space\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings_umap)\n",
    "\n",
    "    # Step 2: Build BERTopic (vectorizer tightened for speed)\n",
    "    vectorizer_model = CountVectorizer(min_df=2, stop_words=\"english\")\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,          # we already provide embeddings\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False,\n",
    "        low_memory=True,\n",
    "    )\n",
    "    # fit once to initialise internal structures\n",
    "    topic_model.fit(texts, embeddings=embeddings)\n",
    "\n",
    "    # Crucial: overwrite clusters with our KMeans labels\n",
    "    topic_model.update_topics(texts, topics=cluster_labels, vectorizer_model=vectorizer_model)\n",
    "\n",
    "    # Step 3: Silhouette on UMAP space\n",
    "    sil = silhouette_score(embeddings_umap, cluster_labels) if len(set(cluster_labels)) > 1 else float(\"nan\")\n",
    "\n",
    "    # Step 4: NPMI over top-10 words per topic\n",
    "    tokenized_texts = [t.split() for t in texts]\n",
    "    top_words = []\n",
    "    for tid, words in topic_model.get_topics().items():\n",
    "        if tid == -1:\n",
    "            continue\n",
    "        top_words.append([w for w, _ in words[:10]])\n",
    "    per_topic_npmi = compute_npmi(top_words, tokenized_texts)\n",
    "    mean_npmi = float(np.nanmean(per_topic_npmi))\n",
    "\n",
    "    results.append({\"k\": k, \"silhouette\": sil, \"npmi\": mean_npmi})\n",
    "    print(f\"K={k}: silhouette={sil:.4f}, npmi={mean_npmi:.4f}\")\n",
    "\n",
    "# choose k by a simple normalised sum of silhouette and NPMI\n",
    "res_df = pd.DataFrame(results)\n",
    "sil_norm = (res_df[\"silhouette\"] - res_df[\"silhouette\"].min()) / (res_df[\"silhouette\"].max() - res_df[\"silhouette\"].min() + 1e-12)\n",
    "npmi_norm = (res_df[\"npmi\"] - res_df[\"npmi\"].min()) / (res_df[\"npmi\"].max() - res_df[\"npmi\"].min() + 1e-12)\n",
    "res_df[\"score\"] = sil_norm + npmi_norm\n",
    "best_k = int(res_df.loc[res_df[\"score\"].idxmax(), \"k\"])\n",
    "print(\"\\nBest k:\", best_k)\n",
    "print(res_df)\n",
    "\n",
    "\n",
    "# ================== 6) Rebuild the final model with best k and save ==================\n",
    "final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "final_labels = final_kmeans.fit_predict(embeddings_umap)\n",
    "\n",
    "final_vectorizer = CountVectorizer(min_df=2, stop_words=\"english\")\n",
    "final_topic_model = BERTopic(\n",
    "    embedding_model=None,\n",
    "    vectorizer_model=final_vectorizer,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    low_memory=True,\n",
    ")\n",
    "final_topic_model.fit(texts, embeddings=embeddings)\n",
    "final_topic_model.update_topics(texts, topics=final_labels, vectorizer_model=final_vectorizer)\n",
    "\n",
    "df[\"topic_kmeans\"] = final_labels\n",
    "topic_info = final_topic_model.get_topic_info().set_index(\"Topic\")[\"Name\"].to_dict()\n",
    "df[\"topic_label_kmeans\"] = df[\"topic_kmeans\"].map(topic_info).fillna(\"Unknown\")\n",
    "\n",
    "df.to_excel(\"articles_with_kmeans_topics.xlsx\", index=False)\n",
    "print(\"Saved final topics to articles_with_kmeans_topics.xlsx\")\n",
    "\n",
    "\n",
    "# ================== 7) (Optional) Plot “Topic Word Scores” ==================\n",
    "def pick_top_topic_ids(topic_model: BERTopic, k: int = 8, by: str = \"Count\"):\n",
    "    info = topic_model.get_topic_info()\n",
    "    info = info[info[\"Topic\"] != -1].sort_values(by=by, ascending=False)\n",
    "    return info[\"Topic\"].head(k).tolist()\n",
    "\n",
    "def plot_topic_word_scores(topic_model: BERTopic, topic_ids=None, top_n_words=5,\n",
    "                           title=\"Topic Word Scores\", ncols=4,\n",
    "                           figsize=(18, 9), outfile=None):\n",
    "    topics_dict = topic_model.get_topics()\n",
    "    if topic_ids is None:\n",
    "        topic_ids = pick_top_topic_ids(topic_model, k=8, by=\"Count\")\n",
    "\n",
    "    n = len(topic_ids)\n",
    "    ncols = min(ncols, n)\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize, squeeze=False)\n",
    "    fig.suptitle(title, fontsize=22, y=1.02)\n",
    "\n",
    "    for i, tid in enumerate(topic_ids):\n",
    "        r, c = divmod(i, ncols)\n",
    "        ax = axes[r, c]\n",
    "        ws = topics_dict.get(tid, [])[:top_n_words]\n",
    "        if not ws:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        words, scores = zip(*ws)\n",
    "        words, scores = list(words)[::-1], list(scores)[::-1]\n",
    "        ax.barh(words, scores)\n",
    "        ax.set_xlim(left=0)\n",
    "        ax.set_title(f\"Topic {tid}\", fontsize=14)\n",
    "        for y, v in enumerate(scores):\n",
    "            ax.text(v, y, f\" {v:.3f}\", va=\"center\", ha=\"left\", fontsize=9)\n",
    "\n",
    "    # turn off any unused subplots\n",
    "    for j in range(i + 1, nrows * ncols):\n",
    "        r, c = divmod(j, ncols)\n",
    "        axes[r, c].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if outfile:\n",
    "        plt.savefig(outfile, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "top_ids = pick_top_topic_ids(final_topic_model, k=8, by=\"Count\")\n",
    "plot_topic_word_scores(final_topic_model, topic_ids=top_ids, top_n_words=5,\n",
    "                       title=\"Topic Word Scores\", outfile=\"topic_word_scores.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d2eef-1088-4580-b5ce-a7277ad4e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Local K-scan\n",
    "\n",
    "#Confirm whether k=35 is the robust optimal value.\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "\n",
    "K_LIST = [30, 32, 35, 38, 40]  # Scan near the optimal valu\n",
    "rows = []\n",
    "for k in K_LIST:\n",
    "    km = KMeans(n_clusters=k, n_init=50, max_iter=500, random_state=42)\n",
    "    lab = km.fit_predict(X_umap)\n",
    "    sil = silhouette_score(X_umap, lab)\n",
    "    ch  = calinski_harabasz_score(X_umap, lab)\n",
    "    db  = davies_bouldin_score(X_umap, lab)\n",
    "    rows.append((k, sil, ch, db))\n",
    "    print(f\"K={k}: silhouette={sil:.4f}, CH={ch:.1f}, DB={db:.4f}\")\n",
    "\n",
    "df_k = pd.DataFrame(rows, columns=[\"k\",\"silhouette\",\"calinski\",\"davies\"])\n",
    "df_k.to_excel(\"k_local_sweep.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc3076-ccf5-4946-a7ec-d4a8440126ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Stability test ,See if the same k=35 is consistent under different initializations or samplings.\n",
    "\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import numpy as np\n",
    "\n",
    "base_labels = KMeans(n_clusters=35, n_init=50, random_state=0).fit_predict(X_umap)\n",
    "\n",
    "# 1) Multiple initializations\n",
    "aris = []\n",
    "for seed in range(1,6):\n",
    "    lab = KMeans(n_clusters=35, n_init=50, random_state=seed).fit_predict(X_umap)\n",
    "    aris.append(adjusted_rand_score(base_labels, lab))\n",
    "print(\"Init reseed stability ARI mean±std:\", np.mean(aris), np.std(aris))\n",
    "\n",
    "# 2) 90% of the sub-samples are resampled\n",
    "rng = np.random.default_rng(42)\n",
    "aris_sub = []\n",
    "for seed in range(5):\n",
    "    idx = rng.choice(len(X_umap), size=int(0.9*len(X_umap)), replace=False)\n",
    "    sub = X_umap[idx]\n",
    "    lab_sub = KMeans(n_clusters=35, n_init=30, random_state=seed).fit_predict(sub)\n",
    "    aris_sub.append(adjusted_rand_score(base_labels[idx], lab_sub))\n",
    "print(\"Bootstrap stability ARI mean±std:\", np.mean(aris_sub), np.std(aris_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee0d96-803b-4514-b526-c5d2f1779bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Quality of topic explainability Verify the semantic robustness of the topic using NPMI and Diversity.\n",
    "# NPMI & Diversity (fast) from an existing topics file\n",
    "# Requirements: numpy, pandas, scikit-learn, openpyxl (for Excel I/O)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load texts and existing topic labels\n",
    "# -----------------------------\n",
    "src_articles = \"ai_articles_with_person_cleaned.xlsx\"\n",
    "src_topics   = \"articles_topics_bestk.xlsx\"  # previously exported file with column 'main_topic'\n",
    "\n",
    "assert os.path.exists(src_articles), f\"Data file not found: {src_articles}\"\n",
    "assert os.path.exists(src_topics),   f\"Topics file not found: {src_topics}\"\n",
    "\n",
    "df_all = pd.read_excel(src_articles)\n",
    "# Keep only articles that have at least one image (as per your workflow)\n",
    "df_all = df_all[df_all[\"num_images\"] > 0].copy()\n",
    "\n",
    "# Keep rows with non-null cleaned text\n",
    "df_txt = df_all.loc[df_all[\"text_cleaned_final\"].notna()].copy()\n",
    "texts  = df_txt[\"text_cleaned_final\"].astype(str).tolist()\n",
    "\n",
    "topics_df = pd.read_excel(src_topics)\n",
    "\n",
    "# If both files carry an article identifier, optionally verify alignment\n",
    "candidate_keys = [\"article_id\", \"id\", \"url_hash\"]\n",
    "for k in candidate_keys:\n",
    "    if k in df_txt.columns and k in topics_df.columns:\n",
    "        assert list(df_txt[k].tolist()) == list(topics_df[k].tolist()), \\\n",
    "            f\"Row order mismatch detected on key '{k}'. Ensure the same ordering before proceeding.\"\n",
    "        break\n",
    "\n",
    "# Assume the topics file was exported in the same order as df_txt\n",
    "assert len(topics_df) == len(texts), f\"Length mismatch: topics={len(topics_df)}, texts={len(texts)}\"\n",
    "labels = topics_df[\"main_topic\"].to_numpy()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Bag-of-words + class-based TF-IDF (c-TF-IDF) to get Top-N words per topic\n",
    "# -----------------------------\n",
    "TOPN = 10\n",
    "vec = CountVectorizer(\n",
    "    min_df=8, max_df=0.9, max_features=20000,\n",
    "    stop_words=\"english\", ngram_range=(1, 1)\n",
    ")\n",
    "X = vec.fit_transform(texts)  # CSR matrix of shape (n_docs, n_terms)\n",
    "vocab = np.array(vec.get_feature_names_out())\n",
    "\n",
    "topics = np.unique(labels)\n",
    "t2r = {t: i for i, t in enumerate(topics)}\n",
    "\n",
    "# Aggregate term counts per topic\n",
    "X_cls = np.zeros((len(topics), X.shape[1]), dtype=np.float64)\n",
    "for t in topics:\n",
    "    idx = np.where(labels == t)[0]\n",
    "    if len(idx):\n",
    "        X_cls[t2r[t]] = np.asarray(X[idx].sum(axis=0)).ravel()\n",
    "\n",
    "# Compute class-based TF-IDF\n",
    "tf = X_cls / (X_cls.sum(axis=1, keepdims=True) + 1e-12)\n",
    "df_term_topics = (X_cls > 0).sum(axis=0)\n",
    "idf = np.log(1.0 + len(topics) / (1.0 + df_term_topics))\n",
    "ctfidf = tf * idf  # shape: (n_topics, n_terms)\n",
    "\n",
    "# Extract Top-N words per topic\n",
    "topic_top_words = {}\n",
    "for t in topics:\n",
    "    r = t2r[t]\n",
    "    if ctfidf[r].sum() == 0:\n",
    "        topic_top_words[t] = []\n",
    "        continue\n",
    "    top_idx = np.argpartition(ctfidf[r], -TOPN)[-TOPN:]\n",
    "    top_idx = top_idx[np.argsort(-ctfidf[r, top_idx])]\n",
    "    topic_top_words[t] = vocab[top_idx].tolist()\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Fast NPMI + Diversity (restricted to the union of Top-k words)\n",
    "# -----------------------------\n",
    "def fast_npmi_and_diversity_from_X(X, topics_words, vec, topk=10):\n",
    "    vocab = vec.get_feature_names_out()\n",
    "    vocab2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "    # Use only the union of each topic's Top-k words to speed up\n",
    "    topic_indices = []\n",
    "    cols_set = set()\n",
    "    for tw in topics_words:\n",
    "        idx = [vocab2idx[w] for w in tw[:topk] if w in vocab2idx]\n",
    "        topic_indices.append(idx)\n",
    "        cols_set.update(idx)\n",
    "\n",
    "    if not cols_set:\n",
    "        return [], 0.0\n",
    "\n",
    "    cols = np.fromiter(sorted(cols_set), dtype=int)\n",
    "    X_sub = X[:, cols].tocsr()\n",
    "\n",
    "    # Binarize and compute co-occurrence counts\n",
    "    B = X_sub.copy()\n",
    "    B.data[:] = 1  # set all nonzero entries to 1\n",
    "    C = (B.T @ B).astype(np.int32).toarray()  # (m x m) co-occurrence\n",
    "    N = X_sub.shape[0]\n",
    "\n",
    "    # Document frequencies and probabilities\n",
    "    dfw = np.diag(C).astype(np.float64)\n",
    "    dfw[dfw == 0] = np.nan\n",
    "    p = dfw / N  # P(w)\n",
    "\n",
    "    # Map original column index -> compact position\n",
    "    pos = {g: i for i, g in enumerate(cols)}\n",
    "    topic_pos = [[pos[g] for g in idxs if g in pos] for idxs in topic_indices]\n",
    "\n",
    "    def npmi_for(idxs):\n",
    "        if len(idxs) < 2:\n",
    "            return float(\"nan\")\n",
    "        vals = []\n",
    "        for i, j in combinations(idxs, 2):\n",
    "            c12 = C[i, j]\n",
    "            if c12 == 0:\n",
    "                continue\n",
    "            p12 = c12 / N\n",
    "            # NPMI: log( p12 / (p1*p2) ) / -log(p12)\n",
    "            v = np.log((p12 + 1e-12) / ((p[i] * p[j]) + 1e-12)) / (-np.log(p12 + 1e-12))\n",
    "            if np.isfinite(v):\n",
    "                vals.append(np.clip(v, -1.0, 1.0))\n",
    "        return float(np.mean(vals)) if vals else float(\"nan\")\n",
    "\n",
    "    npmi_scores = [npmi_for(idxs) for idxs in topic_pos]\n",
    "\n",
    "    # Diversity: unique words over the total number of selected words (topic-wise min(topk, len))\n",
    "    per_topic_counts = [min(topk, len(tw)) for tw in topics_words if tw]\n",
    "    total_selected = int(np.sum(per_topic_counts)) if per_topic_counts else 0\n",
    "    all_words = [w for tw in topics_words for w in tw[:topk]]\n",
    "    diversity = (len(set(all_words)) / total_selected) if total_selected > 0 else 0.0\n",
    "\n",
    "    return npmi_scores, diversity\n",
    "\n",
    "topics_words = [topic_top_words.get(t, []) for t in topics]\n",
    "npmi_vals, div_val = fast_npmi_and_diversity_from_X(X, topics_words, vec, topk=10)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Export per-topic NPMI, Top words, and summary\n",
    "# -----------------------------\n",
    "# Lightweight topic name using the first three top words\n",
    "name3 = {\n",
    "    t: (\", \".join(topic_top_words.get(t, [])[:3]) if topic_top_words.get(t) else \"misc\")\n",
    "    for t in topics\n",
    "}\n",
    "\n",
    "per_topic = pd.DataFrame({\n",
    "    \"topic\": topics,\n",
    "    \"topic_name\": [name3[t] for t in topics],\n",
    "    \"npmi\": npmi_vals,\n",
    "    \"top_words\": [\", \".join(topic_top_words.get(t, [])) for t in topics],\n",
    "    \"size\": [int((labels == t).sum()) for t in topics],\n",
    "}).sort_values(\"topic\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"metric\": [\"NPMI_mean\", \"Diversity\"],\n",
    "    \"value\": [float(np.nanmean(npmi_vals)), float(div_val)]\n",
    "})\n",
    "\n",
    "config = pd.DataFrame({\n",
    "    \"param\": [\"TOPN\", \"Vectorizer.min_df\", \"Vectorizer.max_df\", \"Vectorizer.max_features\",\n",
    "              \"Vectorizer.ngram_range\", \"Timestamp\"],\n",
    "    \"value\": [TOPN, 8, 0.9, 20000, \"(1,1)\", datetime.now().isoformat(timespec=\"seconds\")]\n",
    "})\n",
    "\n",
    "with pd.ExcelWriter(\"topic_quality.xlsx\") as w:\n",
    "    per_topic.to_excel(w, sheet_name=\"per_topic\", index=False)\n",
    "    summary.to_excel(w, sheet_name=\"summary\", index=False)\n",
    "    config.to_excel(w, sheet_name=\"config\", index=False)\n",
    "\n",
    "print(\"Saved -> topic_quality.xlsx\")\n",
    "print(\"NPMI mean:\", float(np.nanmean(npmi_vals)), \"| Diversity:\", float(div_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7f9f0-9ce4-4a75-b744-51c9fd101cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic share over time\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUT_DIR = \"outputs_topic_over_time_yearly\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Load file (support two possible names)\n",
    "candidates = [\"articles_topics_bestk1.xlsx\"]\n",
    "path = next((p for p in candidates if os.path.exists(p)), None)\n",
    "if path is None:\n",
    "    raise FileNotFoundError(\"Could not find articles_topics_bestk.xlsx.\")\n",
    "\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "# 2) Smart column detection\n",
    "def find_col(cols, patterns):\n",
    "    for pat in patterns:\n",
    "        for c in cols:\n",
    "            if re.search(pat, str(c).lower()):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "date_col   = find_col(df.columns, [r\"year\"])\n",
    "topic_col  = find_col(df.columns, [r\"topic_label\"])\n",
    "prob_col   = find_col(df.columns, [r\"prob\", r\"score\", r\"weight\"])\n",
    "bucket_col = find_col(df.columns, [r\"topic[_ ]?bucket\", r\"bucket\"])  # may not exist\n",
    "\n",
    "if date_col is None or topic_col is None:\n",
    "    raise ValueError(f\"Could not automatically find date or topic column. Available columns: {list(df.columns)}\")\n",
    "\n",
    "# 3) Parse date → year\n",
    "dt = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "if dt.isna().all():\n",
    "    # Try year-only\n",
    "    year = df[date_col].astype(str).str.extract(r\"(\\d{4})\")[0]\n",
    "    dt = pd.to_datetime(year + \"-01-01\", errors=\"coerce\")\n",
    "\n",
    "df = df.loc[~dt.isna()].copy()\n",
    "df[\"_year\"] = dt.dt.to_period(\"Y\").dt.to_timestamp()  # year timestamp (Jan 1 each year)\n",
    "\n",
    "# 4) Yearly topic shares\n",
    "df[topic_col] = df[topic_col].astype(str)\n",
    "df[\"_w\"] = pd.to_numeric(df[prob_col], errors=\"coerce\") if prob_col else 1.0\n",
    "df[\"_w\"] = df[\"_w\"].fillna(1.0)\n",
    "\n",
    "counts_topic_year = (\n",
    "    df.pivot_table(index=\"_year\", columns=topic_col, values=\"_w\", aggfunc=\"sum\")\n",
    "    .fillna(0.0)\n",
    "    .sort_index()\n",
    ")\n",
    "shares_topic_year = counts_topic_year.div(\n",
    "    counts_topic_year.sum(axis=1).replace(0, np.nan), axis=0\n",
    ").fillna(0.0)\n",
    "\n",
    "# Choose the top N topics by total weight and plot\n",
    "TOP_N = 8\n",
    "top_topics = counts_topic_year.sum(axis=0).sort_values(ascending=False).head(TOP_N).index\n",
    "plot_mat = shares_topic_year[top_topics]\n",
    "\n",
    "# Save CSVs\n",
    "counts_topic_year.to_csv(os.path.join(OUT_DIR, \"topic_year_counts.csv\"))\n",
    "shares_topic_year.to_csv(os.path.join(OUT_DIR, \"topic_year_shares.csv\"))\n",
    "\n",
    "# 5) Stacked area chart of yearly shares (Top-N topics)\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = plot_mat.index\n",
    "y = plot_mat.values.T  # shape: topics x years\n",
    "plt.stackplot(x, y, labels=top_topics)\n",
    "plt.title(\"Topic share over time (yearly, top 8 topics)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Share\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"topic_over_time_stacked_area_yearly.png\"), dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# 6) If a bucket column exists: line chart of yearly bucket shares\n",
    "if bucket_col:\n",
    "    df[bucket_col] = df[bucket_col].fillna(\"Other\").astype(str)\n",
    "    counts_bucket_year = (\n",
    "        df.pivot_table(index=\"_year\", columns=bucket_col, values=\"_w\", aggfunc=\"sum\")\n",
    "        .fillna(0.0)\n",
    "        .sort_index()\n",
    "    )\n",
    "    shares_bucket_year = counts_bucket_year.div(\n",
    "        counts_bucket_year.sum(axis=1).replace(0, np.nan), axis=0\n",
    "    ).fillna(0.0)\n",
    "\n",
    "    shares_bucket_year.to_csv(os.path.join(OUT_DIR, \"bucket_year_shares.csv\"))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for c in shares_bucket_year.columns:\n",
    "        plt.plot(shares_bucket_year.index, shares_bucket_year[c], marker=\"o\", label=c)\n",
    "    plt.title(\"Bucket share over time (yearly)\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Share\")\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"bucket_over_time_lines_yearly.png\"), dpi=200)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91adb0-b477-49c1-95a1-a3ed2050a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP Theme Map (Color Point Cloud)\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "X = np.load(\"X_umap_5d.npy\")          # (n,5)\n",
    "labels = np.load(\"labels.npy\")        # (n,)\n",
    "xy = X[:, :2]                         # Take the first two dimensions for display\n",
    "topics = np.unique(labels)\n",
    "\n",
    "# Draw a dot\n",
    "plt.figure(figsize=(6,5), dpi=140)\n",
    "scatter = plt.scatter(xy[:,0], xy[:,1], c=labels, s=6, alpha=0.6)\n",
    "# \"Picture Quality Center\"\n",
    "centroids = np.stack([xy[labels==t].mean(axis=0) for t in topics])\n",
    "plt.scatter(centroids[:,0], centroids[:,1], s=60, marker=\"X\", edgecolors=\"k\")\n",
    "\n",
    "plt.title(\"Topic Map (UMAP 2D) with KMeans labels\")\n",
    "plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3909800-2d02-4f5c-8961-e0e8413384ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP topic Map\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from collections import Counter\n",
    "\n",
    "# === Data ===\n",
    "X = np.load(\"X_umap_5d.npy\")[:, :2]\n",
    "labels = np.load(\"labels.npy\")\n",
    "topics = np.unique(labels)\n",
    "\n",
    "# Read short labels per topic (fallback to numeric id if missing)\n",
    "try:\n",
    "    per_topic = pd.read_excel(\"topic_quality.xlsx\", sheet_name=\"per_topic\")\n",
    "    name_map = dict(zip(per_topic[\"topic\"].astype(int),\n",
    "                        per_topic[\"topic_name\"].astype(str)))\n",
    "except Exception:\n",
    "    name_map = {int(t): f\"Topic {int(t)}\" for t in topics}\n",
    "\n",
    "# === Centroids & margin (core vs boundary) ===\n",
    "centroids = np.stack([X[labels == t].mean(axis=0) for t in topics])   # (k, 2)\n",
    "D = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n",
    "Ds = np.sort(D, axis=1)\n",
    "margin = Ds[:, 1] - Ds[:, 0]\n",
    "q10 = np.quantile(margin, 0.10)\n",
    "core = margin > q10\n",
    "\n",
    "# === Color only the top-N largest clusters, others in gray ===\n",
    "N = 10\n",
    "sizes = Counter(labels)\n",
    "top_topics = [t for t, _ in sizes.most_common(N)]\n",
    "other = ~np.isin(labels, top_topics)\n",
    "\n",
    "# Colors\n",
    "base = plt.get_cmap(\"tab20\").colors\n",
    "cmapN = ListedColormap(base[:N])\n",
    "color_of = {t: cmapN(i) for i, t in enumerate(top_topics)}\n",
    "\n",
    "plt.figure(figsize=(7.2, 5.2), dpi=160)\n",
    "\n",
    "# 1) Boundary samples: light gray, bottom layer\n",
    "plt.scatter(X[~core, 0], X[~core, 1], s=6, c=\"#c7c7c7\", alpha=0.15,\n",
    "            rasterized=True, zorder=1, label=\"boundary (10%)\")\n",
    "\n",
    "# 2) Core samples: colored for top-N clusters\n",
    "for t in top_topics:\n",
    "    mask = (labels == t) & core\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], s=8, color=color_of[t], alpha=0.75,\n",
    "                rasterized=True, zorder=3)\n",
    "\n",
    "# 3) Core samples of other clusters: medium gray\n",
    "mask_other_core = other & core\n",
    "plt.scatter(X[mask_other_core, 0], X[mask_other_core, 1], s=6, c=\"#aaaaaa\",\n",
    "            alpha=0.35, rasterized=True, zorder=2)\n",
    "\n",
    "# 4) Centroids & labels (for top-N clusters)\n",
    "def place_label(cx, cy, r=0.18, i=0, n=1):\n",
    "    \"\"\"Evenly distribute label offsets by angle; returns a non-overlapping offset point.\"\"\"\n",
    "    angle = (2 * np.pi * i) / n\n",
    "    return cx + r * np.cos(angle), cy + r * np.sin(angle)\n",
    "\n",
    "cent_map = {int(t): centroids[np.where(topics == t)[0][0]] for t in topics}\n",
    "for idx, t in enumerate(top_topics):\n",
    "    cx, cy = cent_map[int(t)]\n",
    "    # Centroid marker\n",
    "    plt.scatter(cx, cy, s=90, marker=\"X\", edgecolors=\"k\", linewidths=0.9,\n",
    "                facecolors=color_of[t], zorder=5)\n",
    "    # Label: placed around in a ring to avoid stacking\n",
    "    tx, ty = place_label(cx, cy, r=0.22, i=idx, n=len(top_topics))\n",
    "    label_txt = name_map.get(int(t), f\"Topic {int(t)}\")\n",
    "    plt.text(tx, ty, label_txt,\n",
    "             fontsize=8,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.8),\n",
    "             zorder=6)\n",
    "\n",
    "plt.title(\"UMAP Topic Map (k=35): core vs boundary, centroids & labels\", fontsize=12)\n",
    "plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
    "# Keep a short legend entry\n",
    "plt.legend(loc=\"lower right\", frameon=True, fontsize=8)\n",
    "# Slightly expand limits to avoid cutting labels\n",
    "x0, x1 = plt.xlim(); y0, y1 = plt.ylim()\n",
    "plt.xlim(x0 - (x1 - x0) * 0.02, x1 + (x1 - x0) * 0.02)\n",
    "plt.ylim(y0 - (y1 - y0) * 0.02, y1 + (y1 - y0) * 0.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig_topic_map_core_centroids.png\", dpi=300)\n",
    "plt.savefig(\"fig_topic_map_core_centroids.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f03b99-1348-422f-a1ce-d220f663e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#margin distribution\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "X = np.load(\"X_umap_5d.npy\")[:, :2]\n",
    "labels = np.load(\"labels.npy\")\n",
    "topics = np.unique(labels)\n",
    "centers = np.stack([X[labels==t].mean(axis=0) for t in topics])     # Cluster heart\n",
    "dists = np.linalg.norm(X[:,None,:] - centers[None,:,:], axis=2)     # (n,k)\n",
    "margin = np.sort(dists, axis=1)[:,1] - np.sort(dists, axis=1)[:,0]  # Next close - nearest\n",
    "q10 = np.quantile(margin, 0.10)\n",
    "\n",
    "plt.figure(figsize=(6,4), dpi=140)\n",
    "plt.hist(margin, bins=40, alpha=0.8)\n",
    "plt.axvline(q10, linestyle=\"--\")\n",
    "plt.title(\"Margin Distribution (10% cutoff)\")\n",
    "plt.xlabel(\"second-nearest minus nearest centroid distance\"); plt.ylabel(\"count\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df448f37-979c-4e8f-bf8e-4e7a360cf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration comparison of PMI - Diversity (Pareto Scatter)\n",
    "\n",
    "#Content Read four configurations (unigram/bigram/bigram+stop/bigram+stop+core) from topic_quality_compare.xlsx, with NPMI_mean on the horizontal axis and Diversity on the vertical axis, and label roll call.\n",
    "\n",
    "#Function: One diagram proves \"improving the evaluation criteria → simultaneous enhancement of consistency and discrimination\".\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "cmp = pd.read_excel(\"topic_quality_compare.xlsx\", sheet_name=\"summary\")\n",
    "plt.figure(figsize=(6,5), dpi=140)\n",
    "plt.scatter(cmp[\"NPMI_mean\"], cmp[\"Diversity\"], s=60)\n",
    "\n",
    "# Mark the configuration name\n",
    "for _, r in cmp.iterrows():\n",
    "    plt.text(r[\"NPMI_mean\"], r[\"Diversity\"], r[\"config\"])\n",
    "\n",
    "plt.title(\"NPMI vs Diversity across evaluation configs\")\n",
    "plt.xlabel(\"NPMI_mean\"); plt.ylabel(\"Diversity\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177022ac-a5d1-47ab-8120-d5d144c4cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERTopic: Topic Intensity by Year (Top 15 Topics)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Load data\n",
    "file_path = \"articles_topics_bestk1.xlsx\"  # change to your path\n",
    "data = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# 2) Parse date and extract year\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"], errors=\"coerce\")\n",
    "data[\"year\"] = data[\"date\"].dt.year\n",
    "\n",
    "# 3) Select the Top-N topics by total count\n",
    "top_n = 15\n",
    "topic_counts = data[\"topic_label\"].value_counts().head(top_n).index.tolist()\n",
    "\n",
    "# 4) Build the Year × Topic count matrix\n",
    "year_topic_counts = (\n",
    "    data[data[\"topic_label\"].isin(topic_counts)]\n",
    "    .groupby([\"year\", \"topic_label\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Ensure columns are in the same order as 'topic_counts'\n",
    "year_topic_counts = year_topic_counts.reindex(columns=topic_counts)\n",
    "\n",
    "# Optional: save the matrix\n",
    "year_topic_counts.to_csv(\"topic_year_matrix.csv\", index=True)\n",
    "\n",
    "# 5) Plot a heatmap with matplotlib (no seaborn, no explicit colors)\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "im = ax.imshow(year_topic_counts.values, aspect=\"auto\")\n",
    "\n",
    "# Axes and ticks\n",
    "ax.set_xticks(np.arange(len(year_topic_counts.columns)))\n",
    "ax.set_yticks(np.arange(len(year_topic_counts.index)))\n",
    "ax.set_xticklabels(year_topic_counts.columns, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(year_topic_counts.index)\n",
    "\n",
    "ax.set_title(\"BERTopic: Topic Intensity by Year (Top 15 Topics)\")\n",
    "ax.set_xlabel(\"Topic\")\n",
    "ax.set_ylabel(\"Year\")\n",
    "\n",
    "# Colorbar\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Number of Articles\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"topic_year_heatmap.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5b5de-d428-42ec-95e5-1eebe10fe92f",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ffe05-936b-4f4b-90d2-f604b3d5c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load data\n",
    "file_path = \"articles_topics_bestk1.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# 2) Get unique topics\n",
    "topics = (\n",
    "    df[[\"main_topic\", \"topic_label\"]]\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={\"main_topic\": \"topic_id\", \"topic_label\": \"topic_label_full\"})\n",
    ")\n",
    "\n",
    "# 3) Define a three-level category mapping\n",
    "category_map = {\n",
    "    # --- Tech companies & consumer electronics ---\n",
    "    2: (\"Consumer / Tech\", \"Tech Companies & Consumer Electronics\"),\n",
    "    6: (\"Consumer / Tech\", \"Tech Companies & Consumer Electronics\"),\n",
    "    9: (\"Consumer / Tech\", \"Tech Companies & Consumer Electronics\"),\n",
    "    29: (\"Consumer / Tech\", \"Tech Companies & Consumer Electronics\"),\n",
    "    33: (\"Consumer / Tech\", \"Tech Companies & Consumer Electronics\"),\n",
    "    34: (\"Consumer / Tech\", \"Tech Companies & Consumer Electronics\"),\n",
    "\n",
    "    # --- AI research & companies ---\n",
    "    3: (\"Consumer / Tech\", \"AI Research & Companies\"),\n",
    "    4: (\"Culture / Media\", \"AI in Culture / Deepfake / Copyright\"),\n",
    "    8: (\"Consumer / Tech\", \"AI Research & Companies\"),\n",
    "    11: (\"Culture / Media\", \"AI in Culture / Deepfake / Copyright\"),\n",
    "    15: (\"Consumer / Tech\", \"AI Research & Companies\"),\n",
    "\n",
    "    # --- Media & culture ---\n",
    "    7: (\"Culture / Media\", \"Media & Culture\"),\n",
    "    20: (\"Culture / Media\", \"Media & Culture\"),\n",
    "    32: (\"Culture / Media\", \"Media & Culture\"),\n",
    "\n",
    "    # --- Politics & governance ---\n",
    "    0: (\"Politics / Society\", \"Politics & Governance\"),\n",
    "    10: (\"Politics / Society\", \"Politics & Governance\"),\n",
    "    12: (\"Politics / Society\", \"Politics & Governance\"),\n",
    "    16: (\"Politics / Society\", \"Politics & Governance\"),\n",
    "    17: (\"Politics / Society\", \"Politics & Governance\"),\n",
    "    18: (\"Politics / Society\", \"Politics & Governance\"),\n",
    "    23: (\"Politics / Society\", \"Politics & Governance\"),\n",
    "\n",
    "    # --- War & security ---\n",
    "    25: (\"Politics / Society\", \"War & Security\"),\n",
    "    26: (\"Politics / Society\", \"War & Security\"),\n",
    "\n",
    "    # --- Economy & finance ---\n",
    "    24: (\"Consumer / Tech\", \"Economy & Finance\"),\n",
    "    28: (\"Consumer / Tech\", \"Economy & Finance\"),\n",
    "    30: (\"Consumer / Tech\", \"Economy & Finance\"),\n",
    "    31: (\"Consumer / Tech\", \"Economy & Finance\"),\n",
    "\n",
    "    # --- Science & future exploration ---\n",
    "    1: (\"Consumer / Tech\", \"Science & Future Exploration\"),\n",
    "    21: (\"Consumer / Tech\", \"Science & Future Exploration\"),\n",
    "    22: (\"Consumer / Tech\", \"Science & Future Exploration\"),\n",
    "    27: (\"Consumer / Tech\", \"Science & Future Exploration\"),\n",
    "}\n",
    "\n",
    "# 4) Map the three-level categories\n",
    "topics[\"big_category\"] = topics[\"topic_id\"].map(\n",
    "    lambda x: category_map.get(x, (\"Consumer / Tech\", \"Other\"))[0]\n",
    ")\n",
    "topics[\"mid_category\"] = topics[\"topic_id\"].map(\n",
    "    lambda x: category_map.get(x, (\"Consumer / Tech\", \"Other\"))[1]\n",
    ")\n",
    "\n",
    "# 5) Prepare the output table\n",
    "hierarchy = topics[[\"big_category\", \"mid_category\", \"topic_id\", \"topic_label_full\"]].sort_values(\n",
    "    by=[\"big_category\", \"mid_category\", \"topic_id\"]\n",
    ")\n",
    "\n",
    "# 6) Save as CSV\n",
    "csv_out = \"topic_hierarchy.csv\"\n",
    "hierarchy.to_csv(csv_out, index=False)\n",
    "\n",
    "# 7) Show a quick preview and the output path\n",
    "hierarchy.head(20), csv_out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
