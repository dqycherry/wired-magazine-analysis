{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23be8a8-c074-45a9-b354-ff132c65506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4 (sentence-level): stance by year with VADER\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "INPUT_XLSX = \"articles_processed.xlsx\"          # must contain Year + text\n",
    "OUT_DIR = \"outputs_wired_text\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_XLSX         = os.path.join(OUT_DIR, \"stage4_sentence_level_stance_by_year.xlsx\")\n",
    "OUT_PNG_COUNTS   = os.path.join(OUT_DIR, \"stage4_sentence_level_counts.png\")\n",
    "OUT_PNG_SHARES   = os.path.join(OUT_DIR, \"stage4_sentence_level_shares.png\")\n",
    "OUT_PNG_STRONG   = os.path.join(OUT_DIR, \"stage4_sentence_level_strongshare_mean.png\")\n",
    "\n",
    "# ---------------- Load -----------------\n",
    "df = pd.read_excel(INPUT_XLSX)\n",
    "\n",
    "# pick text column\n",
    "TEXT_CANDIDATES = [\"text_cleaned_final\", \"text\"]\n",
    "text_col = next((c for c in TEXT_CANDIDATES if c in df.columns), None)\n",
    "if text_col is None:\n",
    "    raise ValueError(\"No text column found. Need 'text_cleaned_final' or 'text' in articles_processed.xlsx.\")\n",
    "\n",
    "# ensure Year exists and is numeric\n",
    "if \"Year\" not in df.columns:\n",
    "    raise ValueError(\"Input must contain a 'Year' column (run Stage 1 first).\")\n",
    "df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df = df[df[\"Year\"].notna()].copy()\n",
    "\n",
    "# ---------------- NLTK resources ----------\n",
    "for pkg, path in [(\"vader_lexicon\",\"sentiment/vader_lexicon.zip\"),\n",
    "                  (\"punkt\",\"tokenizers/punkt\")]:\n",
    "    try:\n",
    "        nltk.data.find(path)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def clean_spaces(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "    return s\n",
    "\n",
    "def sentence_level_scores(text: str,\n",
    "                          strong_thresh: float = 0.5) -> dict:\n",
    "    \"\"\"\n",
    "    Return per-article aggregates derived from sentence polarity.\n",
    "    - sent_mean: length-weighted mean compound over sentences\n",
    "    - strong_share: share of strong polarity sentences (|compound|>=strong_thresh)\n",
    "    - n_sent: #sentences used\n",
    "    \"\"\"\n",
    "    txt = clean_spaces(text)\n",
    "    if not txt:\n",
    "        return {\"sent_mean\": np.nan, \"strong_share\": np.nan, \"n_sent\": 0}\n",
    "\n",
    "    sents = [s.strip() for s in sent_tokenize(txt) if s.strip()]\n",
    "    if not sents:\n",
    "        return {\"sent_mean\": np.nan, \"strong_share\": np.nan, \"n_sent\": 0}\n",
    "\n",
    "    comps, lengths, strong_flags = [], [], []\n",
    "    for s in sents:\n",
    "        comp = sia.polarity_scores(s)[\"compound\"]\n",
    "        # token length as weight (min 1)\n",
    "        L = max(1, len(word_tokenize(s)))\n",
    "        comps.append(comp); lengths.append(L)\n",
    "        strong_flags.append(1 if abs(comp) >= strong_thresh else 0)\n",
    "\n",
    "    weights = np.array(lengths, dtype=float)\n",
    "    weights = weights / weights.sum()\n",
    "    sent_mean = float(np.dot(np.array(comps, dtype=float), weights))\n",
    "    strong_share = float(np.mean(strong_flags))\n",
    "    return {\"sent_mean\": sent_mean, \"strong_share\": strong_share, \"n_sent\": len(sents)}\n",
    "\n",
    "# ---------------- Compute per-article aggregates ----------------\n",
    "agg_rows = df[text_col].astype(str).apply(sentence_level_scores).apply(pd.Series)\n",
    "df = pd.concat([df.reset_index(drop=True), agg_rows.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Article stance from sentence-level mean\n",
    "# thresholds can be tuned; these are standard for VADER:\n",
    "NEG_T = -0.05\n",
    "POS_T =  0.05\n",
    "def to_stance(x):\n",
    "    if pd.isna(x): return \"neutral\"\n",
    "    if x > POS_T:  return \"positive\"\n",
    "    if x < NEG_T:  return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "df[\"stance\"] = df[\"sent_mean\"].apply(to_stance)\n",
    "\n",
    "# ---------------- Yearly aggregation ----------------\n",
    "# counts per stance\n",
    "stance_counts = (\n",
    "    df.groupby([\"Year\", \"stance\"])\n",
    "      .size().reset_index(name=\"count\")\n",
    "      .pivot(index=\"Year\", columns=\"stance\", values=\"count\")\n",
    "      .fillna(0)\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# shares per year (row-normalized)\n",
    "stance_shares = stance_counts.copy()\n",
    "row_sum = stance_shares.drop(columns=[\"Year\"]).sum(axis=1).replace(0, 1)\n",
    "for col in [c for c in stance_shares.columns if c != \"Year\"]:\n",
    "    stance_shares[col] = stance_shares[col] / row_sum\n",
    "\n",
    "# mean strong polarity share by year (how opinionated)\n",
    "strong_share_year = (\n",
    "    df.groupby(\"Year\")[\"strong_share\"]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"strong_share\":\"mean_strong_share\"})\n",
    "      .sort_values(\"Year\")\n",
    ")\n",
    "\n",
    "# ---------------- Save Excel ----------------\n",
    "with pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\") as writer:\n",
    "    df[[\"Year\",\"sent_mean\",\"strong_share\",\"n_sent\",\"stance\"]].to_excel(writer, sheet_name=\"per_article\", index=False)\n",
    "    stance_counts.to_excel(writer, sheet_name=\"counts\", index=False)\n",
    "    stance_shares.to_excel(writer, sheet_name=\"shares\", index=False)\n",
    "    strong_share_year.to_excel(writer, sheet_name=\"strong_share_year\", index=False)\n",
    "\n",
    "print(\"Stage 4 (sentence-level) done ->\", OUT_XLSX)\n",
    "\n",
    "# ---------------- Plot: counts ----------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "for col in [c for c in stance_counts.columns if c != \"Year\"]:\n",
    "    plt.plot(stance_counts[\"Year\"], stance_counts[col], marker=\"o\", label=col)\n",
    "plt.title(\"AI article stance by year (sentence-level, counts)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Number of articles\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "plt.savefig(OUT_PNG_COUNTS, dpi=300); plt.close()\n",
    "\n",
    "# ---------------- Plot: shares ----------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "for col in [c for c in stance_shares.columns if c != \"Year\"]:\n",
    "    plt.plot(stance_shares[\"Year\"], stance_shares[col], marker=\"o\", label=col)\n",
    "plt.title(\"AI article stance by year (sentence-level, shares)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Share of articles\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "plt.savefig(OUT_PNG_SHARES, dpi=300); plt.close()\n",
    "\n",
    "# ---------------- Plot: opinion strength (mean strong_share) ----------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(strong_share_year[\"Year\"], strong_share_year[\"mean_strong_share\"], marker=\"o\")\n",
    "plt.title(\"Mean share of strong-polarity sentences by year (|compound| ≥ 0.5)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"Mean strong sentence share\")\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "plt.savefig(OUT_PNG_STRONG, dpi=300); plt.close()\n",
    "\n",
    "print(\"PNGs saved ->\")\n",
    "print(\"  Counts:\", OUT_PNG_COUNTS)\n",
    "print(\"  Shares:\", OUT_PNG_SHARES)\n",
    "print(\"  Strong-share:\", OUT_PNG_STRONG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc6750-63d7-41f9-8cf2-c8de86efc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Sensitivity Analysis\n",
    "\"\"\"\n",
    "Sentiment Sensitivity Check (dual-rule)\n",
    "--------------------------------------\n",
    "Purpose:\n",
    "- Given article-level sentiment metrics (sent_mean, strong_share),\n",
    "  sweep neutral-band & strong-max parameters and report label shares\n",
    "  and change rate vs a baseline. Optionally recompute metrics from text.\n",
    "\n",
    "Inputs (choose one):\n",
    "A) Preferred: Excel with an \"article_level\" sheet (from a prior run)\n",
    "   containing columns: Year, sent_mean, strong_share\n",
    "B) If A is not available: provide a text column and Year, and this script\n",
    "   will recompute sent_mean & strong_share using VADER sentence scoring.\n",
    "\n",
    "Outputs (in OUT_DIR):\n",
    "- overall_label_shares_by_grid.xlsx / .csv\n",
    "- label_change_rate_vs_baseline.xlsx / .csv\n",
    "- shares_by_year_(grid).xlsx  (one sheet per parameter combo for year-by-year robustness checks)\n",
    "- heatmap_overall_neutral_share.png\n",
    "- heatmap_changed_vs_baseline.png\n",
    "\"\"\"\n",
    "\n",
    "# --- Requirements: pandas, numpy, matplotlib, openpyxl (Excel), nltk (vader_lexicon, punkt) ---\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====================== CONFIG ======================\n",
    "# Prefer using your previously exported article-level details (with sent_mean/strong_share)\n",
    "INPUT_XLSX          = \"outputs_wired_text_dualmetric/stance_dual_by_year.xlsx\"\n",
    "ARTICLE_SHEET_NAME  = \"article_level\"   # change to your sheet name\n",
    "\n",
    "# If you need to recompute from text (when article_level lacks sent_mean/strong_share)\n",
    "# Candidate column names (case-insensitive)\n",
    "TEXT_CANDIDATES = [\"text_cleaned_final\", \"text\", \"content\", \"article_text\", \"body\"]\n",
    "YEAR_CANDIDATES = [\"Year\", \"year\", \"pub_year\", \"year_pub\"]\n",
    "\n",
    "# Parameter grids (modifiable)\n",
    "NEUTRAL_BAND_GRID = [0.10, 0.15, 0.20]\n",
    "STRONG_MAX_GRID   = [0.15, 0.20, 0.25]\n",
    "\n",
    "# Baseline parameters (for change-rate calculation)\n",
    "BASE_NEUTRAL_BAND = 0.15\n",
    "BASE_STRONG_MAX   = 0.20\n",
    "\n",
    "# Sentence-level \"strong polarity\" threshold (VADER convention)\n",
    "STRONG_SENT_THRESH = 0.5\n",
    "\n",
    "# Output directory\n",
    "OUT_DIR = \"sentiment_sensitivity_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Whether to export Excel files (CSV is always exported)\n",
    "SAVE_EXCEL = True\n",
    "# ===================================================\n",
    "\n",
    "\n",
    "def pick_col(df, candidates, must=True, desc=\"\"):\n",
    "    cols_map = {str(c).strip().lower(): c for c in df.columns}\n",
    "    for name in candidates:\n",
    "        if name.lower() in cols_map:\n",
    "            return cols_map[name.lower()]\n",
    "    if must:\n",
    "        raise KeyError(\n",
    "            f\"Missing required column for {desc}. \"\n",
    "            f\"Tried {candidates}. Available: {list(df.columns)}\"\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def try_load_article_level(path, sheet):\n",
    "    \"\"\"Prefer loading your previously exported article_level (contains sent_mean/strong_share).\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_excel(path, sheet_name=sheet)\n",
    "    except Exception:\n",
    "        return None\n",
    "    cols = [str(c) for c in df.columns]\n",
    "    if not {\"sent_mean\", \"strong_share\"}.issubset(set(cols)):\n",
    "        return None\n",
    "    # Normalize Year\n",
    "    year_col = pick_col(df, YEAR_CANDIDATES, must=True, desc=\"Year\")\n",
    "    df[year_col] = pd.to_numeric(df[year_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[year_col]).copy()\n",
    "    df.rename(columns={year_col: \"Year\"}, inplace=True)\n",
    "    return df[[\"Year\", \"sent_mean\", \"strong_share\"]].copy()\n",
    "\n",
    "\n",
    "def recompute_from_text(path, sheet_or_none=None):\n",
    "    \"\"\"When article_level lacks sent_mean/strong_share, recompute from text.\"\"\"\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "    for res in [\"vader_lexicon\", \"punkt\"]:\n",
    "        try:\n",
    "            nltk.data.find(f\"tokenizers/{res}\" if res == \"punkt\" else f\"sentiment/{res}\")\n",
    "        except LookupError:\n",
    "            nltk.download(res)\n",
    "\n",
    "    if sheet_or_none is None:\n",
    "        df0 = pd.read_excel(path)\n",
    "    else:\n",
    "        df0 = pd.read_excel(path, sheet_name=sheet_or_none)\n",
    "\n",
    "    text_col = pick_col(df0, TEXT_CANDIDATES, must=True, desc=\"text\")\n",
    "    year_col = pick_col(df0, YEAR_CANDIDATES, must=True, desc=\"year\")\n",
    "\n",
    "    df0[year_col] = pd.to_numeric(df0[year_col], errors=\"coerce\")\n",
    "    df0 = df0.dropna(subset=[year_col]).copy()\n",
    "    df0.rename(columns={year_col: \"Year\"}, inplace=True)\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def sent_metrics(txt):\n",
    "        t = str(txt).strip()\n",
    "        if not t:\n",
    "            return pd.Series({\"sent_mean\": np.nan, \"strong_share\": np.nan})\n",
    "        sents = [s.strip() for s in sent_tokenize(t) if s.strip()]\n",
    "        if not sents:\n",
    "            return pd.Series({\"sent_mean\": np.nan, \"strong_share\": np.nan})\n",
    "        scores = [sia.polarity_scores(s)[\"compound\"] for s in sents]\n",
    "        sent_mean = float(np.mean(scores))\n",
    "        strong_share = float(np.mean([abs(v) >= STRONG_SENT_THRESH for v in scores]))\n",
    "        return pd.Series({\"sent_mean\": sent_mean, \"strong_share\": strong_share})\n",
    "\n",
    "    agg = df0[text_col].astype(str).apply(sent_metrics)\n",
    "    df = pd.concat([df0[[\"Year\"]].reset_index(drop=True), agg.reset_index(drop=True)], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def dual_label(m, s, neutral_band, strong_max):\n",
    "    if np.isnan(m) or np.isnan(s):\n",
    "        return \"unknown\"\n",
    "    if -neutral_band <= m <= neutral_band and s < strong_max:\n",
    "        return \"neutral\"\n",
    "    return \"positive\" if m > 0 else \"negative\"\n",
    "\n",
    "\n",
    "def compute_tables(df, neutral_grid, strong_grid, base_nb, base_sm):\n",
    "    \"\"\"Produce two overall tables + a dict of year-by-year share tables for each parameter combo.\"\"\"\n",
    "    # baseline labels\n",
    "    base_labels = df.apply(lambda r: dual_label(r[\"sent_mean\"], r[\"strong_share\"], base_nb, base_sm), axis=1)\n",
    "\n",
    "    overall_rows = []\n",
    "    change_rows = []\n",
    "    by_year_tables = {}  # (nb, sm) -> DataFrame(year x label shares%)\n",
    "\n",
    "    # for heatmaps\n",
    "    neutral_share_map = np.zeros((len(neutral_grid), len(strong_grid)))\n",
    "    change_rate_map = np.zeros((len(neutral_grid), len(strong_grid)))\n",
    "\n",
    "    for i, nb in enumerate(neutral_grid):\n",
    "        for j, sm in enumerate(strong_grid):\n",
    "            labels = df.apply(lambda r: dual_label(r[\"sent_mean\"], r[\"strong_share\"], nb, sm), axis=1)\n",
    "\n",
    "            # overall shares\n",
    "            share = (labels.value_counts(normalize=True) * 100).reindex(\n",
    "                [\"positive\", \"neutral\", \"negative\", \"unknown\"]\n",
    "            ).fillna(0)\n",
    "\n",
    "            overall_rows.append({\n",
    "                \"neutral_band\": nb,\n",
    "                \"strong_max\": sm,\n",
    "                \"positive_%\": round(share[\"positive\"], 2),\n",
    "                \"neutral_%\": round(share[\"neutral\"], 2),\n",
    "                \"negative_%\": round(share[\"negative\"], 2),\n",
    "                \"unknown_%\": round(share[\"unknown\"], 2),\n",
    "            })\n",
    "\n",
    "            # change vs baseline\n",
    "            change_rate = float((labels != base_labels).mean()) * 100.0\n",
    "            change_rows.append({\n",
    "                \"neutral_band\": nb,\n",
    "                \"strong_max\": sm,\n",
    "                \"changed_vs_baseline_%\": round(change_rate, 2)\n",
    "            })\n",
    "\n",
    "            # by-year shares table\n",
    "            tmp = pd.DataFrame({\"Year\": df[\"Year\"].values, \"label\": labels.values})\n",
    "            by_year = (tmp.groupby([\"Year\", \"label\"]).size().reset_index(name=\"count\")\n",
    "                         .pivot(index=\"Year\", columns=\"label\", values=\"count\")\n",
    "                         .fillna(0).sort_index())\n",
    "            by_year_shares = by_year.div(by_year.sum(axis=1).replace(0, 1), axis=0) * 100\n",
    "            by_year_tables[(nb, sm)] = by_year_shares.reindex(\n",
    "                columns=[\"positive\", \"neutral\", \"negative\", \"unknown\"]\n",
    "            ).fillna(0)\n",
    "\n",
    "            # heatmap values\n",
    "            neutral_share_map[i, j] = share[\"neutral\"]\n",
    "            change_rate_map[i, j] = change_rate\n",
    "\n",
    "    overall_table = pd.DataFrame(overall_rows).sort_values([\"neutral_band\", \"strong_max\"]).reset_index(drop=True)\n",
    "    change_table = pd.DataFrame(change_rows).sort_values([\"neutral_band\", \"strong_max\"]).reset_index(drop=True)\n",
    "    return overall_table, change_table, by_year_tables, neutral_share_map, change_rate_map\n",
    "\n",
    "\n",
    "def plot_heatmap(matrix, x_labels, y_labels, title, out_png):\n",
    "    plt.figure(figsize=(6, 4.5))\n",
    "    plt.imshow(matrix, aspect=\"auto\")\n",
    "    plt.colorbar(label=\"value\")\n",
    "    plt.xticks(range(len(x_labels)), x_labels)\n",
    "    plt.yticks(range(len(y_labels)), y_labels)\n",
    "    plt.xlabel(\"STRONG_MAX\")\n",
    "    plt.ylabel(\"NEUTRAL_BAND\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) Try loading article_level (with sent_mean/strong_share)\n",
    "    df = try_load_article_level(INPUT_XLSX, ARTICLE_SHEET_NAME)\n",
    "    if df is None:\n",
    "        # 2) If not available, recompute from text (same file or another with a text column)\n",
    "        df = recompute_from_text(INPUT_XLSX, ARTICLE_SHEET_NAME)\n",
    "\n",
    "    # Clean Year\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Year\"]).copy()\n",
    "\n",
    "    # 3) Compute sensitivity tables\n",
    "    shares, changes, by_year, neutral_map, change_map = compute_tables(\n",
    "        df, NEUTRAL_BAND_GRID, STRONG_MAX_GRID, BASE_NEUTRAL_BAND, BASE_STRONG_MAX\n",
    "    )\n",
    "\n",
    "    # 4) Save tables\n",
    "    shares_csv = os.path.join(OUT_DIR, \"overall_label_shares_by_grid.csv\")\n",
    "    changes_csv = os.path.join(OUT_DIR, \"label_change_rate_vs_baseline.csv\")\n",
    "    shares.to_csv(shares_csv, index=False)\n",
    "    changes.to_csv(changes_csv, index=False)\n",
    "\n",
    "    if SAVE_EXCEL:\n",
    "        with pd.ExcelWriter(os.path.join(OUT_DIR, \"overall_label_shares_by_grid.xlsx\")) as w:\n",
    "            shares.to_excel(w, sheet_name=\"overall_shares\", index=False)\n",
    "        with pd.ExcelWriter(os.path.join(OUT_DIR, \"label_change_rate_vs_baseline.xlsx\")) as w:\n",
    "            changes.to_excel(w, sheet_name=\"changes_vs_baseline\", index=False)\n",
    "        # One “by year” sheet per parameter combo—handy for citing specific grid-year curves\n",
    "        by_year_book = os.path.join(OUT_DIR, \"shares_by_year_all_grids.xlsx\")\n",
    "        with pd.ExcelWriter(by_year_book) as w:\n",
    "            for (nb, sm), tab in by_year.items():\n",
    "                sheet = f\"nb_{str(nb).replace('.','_')}_sm_{str(sm).replace('.','_')}\"\n",
    "                tab.reset_index().to_excel(w, sheet_name=sheet[:31], index=False)\n",
    "\n",
    "    # 5) Heatmaps (overview)\n",
    "    plot_heatmap(\n",
    "        neutral_map, STRONG_MAX_GRID, NEUTRAL_BAND_GRID,\n",
    "        \"Neutral share (%) by parameter grid\",\n",
    "        os.path.join(OUT_DIR, \"heatmap_overall_neutral_share.png\")\n",
    "    )\n",
    "    plot_heatmap(\n",
    "        change_map, STRONG_MAX_GRID, NEUTRAL_BAND_GRID,\n",
    "        \"Label change rate (%) vs baseline\",\n",
    "        os.path.join(OUT_DIR, \"heatmap_changed_vs_baseline.png\")\n",
    "    )\n",
    "\n",
    "    # 6) Console preview\n",
    "    print(\"\\n=== Overall shares by parameter grid ===\")\n",
    "    print(shares.to_string(index=False))\n",
    "    print(\"\\n=== Change rate vs baseline (%) ===\")\n",
    "    print(changes.to_string(index=False))\n",
    "    print(f\"\\nOutputs saved in: {OUT_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
