{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085b9c8a-064b-41c4-8ed9-d87475e0b000",
   "metadata": {},
   "source": [
    "section 1 compare clip and yolo, choose yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e9bbd-b46d-4cf7-8e5d-02b9e03b08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e6b20-d7b3-4b2a-8644-769d11416b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter contrib nbextension install --user\n",
    "!pip install pandas requests beautifulsoup4 python-dateutil openpyxl tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5504cff-a590-47a9-970e-8676624b0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "\n",
    "# Load input Excel file\n",
    "input_path = \"ai_articles_only.xlsx\"\n",
    "df = pd.read_excel(input_path)\n",
    "urls = df['article_url'].dropna().unique()\n",
    "\n",
    "# Extract readable text from article\n",
    "def extract_text(soup):\n",
    "    article = soup.find(\"article\") or soup\n",
    "    tags = article.find_all([\"p\", \"h2\", \"h3\", \"li\", \"blockquote\"])\n",
    "    text = \"\\n\".join(\n",
    "        t.get_text(strip=True) for t in tags\n",
    "        if len(t.get_text(strip=True)) > 20\n",
    "    )\n",
    "    return text.strip()\n",
    "\n",
    "# Extract image URLs from <article>\n",
    "def extract_article_images(soup):\n",
    "    article = soup.find(\"article\")\n",
    "    img_tags = article.find_all(\"img\") if article else []\n",
    "    return [\n",
    "        img.get(\"src\") for img in img_tags\n",
    "        if img.get(\"src\") and img.get(\"src\").startswith(\"http\")\n",
    "    ]\n",
    "\n",
    "# Clean illegal characters for Excel\n",
    "def clean_excel_string(s):\n",
    "    if isinstance(s, str):\n",
    "        return re.sub(r\"[\\x00-\\x1F\\x7F]\", \"\", s)\n",
    "    return s\n",
    "\n",
    "# Fetch one article\n",
    "def fetch_article(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        title = soup.title.text.strip() if soup.title else \"\"\n",
    "\n",
    "        author_tag = soup.find(\"meta\", {\"name\": \"author\"})\n",
    "        author = author_tag[\"content\"].strip() if author_tag else \"\"\n",
    "\n",
    "        date_tag = soup.find(\"time\")\n",
    "        if date_tag and date_tag.get(\"datetime\"):\n",
    "            try:\n",
    "                raw_date = date_tag.get(\"datetime\")\n",
    "                date = parse(raw_date).strftime(\"%Y-%m\")\n",
    "            except:\n",
    "                date = raw_date\n",
    "        else:\n",
    "            date = \"\"\n",
    "\n",
    "        text = extract_text(soup)\n",
    "        image_urls = extract_article_images(soup)\n",
    "        num_images = len(image_urls)\n",
    "\n",
    "        return {\n",
    "            \"article_url\": url,\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"text\": text,\n",
    "            \"num_images\": num_images,\n",
    "            \"image_urls\": image_urls[:10]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"article_url\": url,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Crawl all articles with threads\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    futures = {executor.submit(fetch_article, url): url for url in urls}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Crawling articles\"):\n",
    "        results.append(future.result())\n",
    "\n",
    "# Clean and save to Excel\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df = result_df.applymap(clean_excel_string)\n",
    "result_df.to_excel(\"article_metadata.xlsx\", index=False)\n",
    "print(\"Done. Saved to article_metadata.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ebf0c-c0d0-452e-a7d5-3c88ddf7dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is generated after excluding many related stories and irrelevant pictures such as advertisements and needs to be retained. Neither of the first two needs to be retained.\n",
    "import pandas as pd\n",
    "\n",
    "# === Step 1: Load metadata ===\n",
    "input_path = \"article_metadata_filtered.xlsx\"  # Replace if needed\n",
    "df = pd.read_excel(input_path)\n",
    "\n",
    "# === Step 2: Filter out articles with no images ===\n",
    "df = df[df['num_images'] > 0].reset_index(drop=True)\n",
    "\n",
    "# === Step 3: Expand image_urls list ===\n",
    "rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    article_url = row['article_url']\n",
    "    article_date = row['date']\n",
    "    \n",
    "    try:\n",
    "        image_list = eval(row['image_urls']) if isinstance(row['image_urls'], str) else row['image_urls']\n",
    "    except:\n",
    "        image_list = []\n",
    "\n",
    "    for i, img_url in enumerate(image_list):\n",
    "        rows.append({\n",
    "            \"article_id\": article_url,\n",
    "            \"image_id\": f\"img{i}\",\n",
    "            \"image_path\": img_url,\n",
    "            \"date\": article_date,\n",
    "            \"has_person\": \"\",\n",
    "            \"race_image\": \"\",\n",
    "            \"gender_image\": \"\",\n",
    "            \"role_image\": \"\",\n",
    "            \"identity_label\": \"\",\n",
    "            \"simulated_human\": \"\"\n",
    "        })\n",
    "\n",
    "# === Step 4: Save to Excel ===\n",
    "df_images = pd.DataFrame(rows)\n",
    "df_images.to_excel(\"image_annotation_extended_template.xlsx\", index=False)\n",
    "print(\"Done! Saved to generated_image_annotation_template.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed0204-8677-4434-ad23-5c9c8fcea25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# === Step 1: Load your Excel ===\n",
    "input_path = \"/Users/dengqiuyue/Downloads/final project/scraping/final_merged_metadata_with_labels.xlsx\"\n",
    "df = pd.read_excel(input_path)\n",
    "\n",
    "# === Step 2: Setup\n",
    "base_folder = \"images\"\n",
    "os.makedirs(base_folder, exist_ok=True)\n",
    "df[\"local_path\"] = \"\"\n",
    "df[\"has_person_clip\"] = \"\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "texts = [\"a photo of a person\", \"a photo without any people\"]\n",
    "\n",
    "# === Step 3: Load CLIP model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# === Step 4: Define thread task\n",
    "def process_image(row_idx, row):\n",
    "    try:\n",
    "        url = str(row[\"image_path\"]).strip()\n",
    "        article_id = str(row[\"article_id\"]).replace(\"/\", \"_\")\n",
    "        image_id = str(row[\"image_id\"])\n",
    "        article_folder = os.path.join(base_folder, article_id)\n",
    "        os.makedirs(article_folder, exist_ok=True)\n",
    "\n",
    "        filename = f\"{image_id}.jpg\"\n",
    "        save_path = os.path.join(article_folder, filename)\n",
    "\n",
    "        # Download\n",
    "        if not os.path.exists(save_path):\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        # CLIP prediction\n",
    "        image = Image.open(save_path)\n",
    "        if image.mode in [\"P\", \"LA\", \"RGBA\"]:\n",
    "            image = image.convert(\"RGB\")\n",
    "        inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        probs = outputs.logits_per_image.softmax(dim=1).squeeze()\n",
    "        prediction = \"yes\" if probs[0] > probs[1] else \"no\"\n",
    "\n",
    "        return (row_idx, save_path, prediction)\n",
    "\n",
    "    except Exception as e:\n",
    "        return (row_idx, \"download_error\", \"error\")\n",
    "\n",
    "# === Step 5: Run multithreaded\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    futures = [executor.submit(process_image, idx, row) for idx, row in df.iterrows()]\n",
    "    for future in tqdm(futures, desc=\"Downloading + CLIP Judging\"):\n",
    "        idx, path, result = future.result()\n",
    "        df.at[idx, \"local_path\"] = path\n",
    "        df.at[idx, \"has_person_clip\"] = result\n",
    "\n",
    "# === Step 6: Save result\n",
    "output_path = \"image_annotation_with_clip_multithreaded.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Done! Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42e04f-f49c-4e64-92df-b5e684658219",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594522a-5bfb-405b-b967-9361816372dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# === Step 1: Load Excel ===\n",
    "input_path = \"/Users/dengqiuyue/Downloads/final project/scraping/final_merged_metadata_with_labels.xlsx\"\n",
    "df = pd.read_excel(input_path)\n",
    "df[\"local_path\"] = \"\"\n",
    "\n",
    "# === Step 2: Prepare base folder ===\n",
    "base_folder = \"final project code/image1\"\n",
    "os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "# === Step 3: Clean article_url to valid folder path\n",
    "def safe_folder_name(article_url):\n",
    "    return (\n",
    "        str(article_url)\n",
    "        .replace(\"https://\", \"https___\")\n",
    "        .replace(\"http://\", \"http___\")\n",
    "        .replace(\"/\", \"_\")\n",
    "        .replace(\":\", \"_\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "# === Step 4: Download function\n",
    "def download_image(row):\n",
    "    try:\n",
    "        url = str(row[\"image_path\"]).strip()\n",
    "        folder_name = safe_folder_name(row[\"article_id\"])\n",
    "        image_id = str(row[\"image_id\"]).replace(\"/\", \"_\")\n",
    "\n",
    "        folder = os.path.join(base_folder, folder_name)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        save_path = os.path.join(folder, f\"{image_id}.jpg\")\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "            r = requests.get(url, headers=headers, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "        return save_path\n",
    "    except Exception as e:\n",
    "        return \"download_error\"\n",
    "\n",
    "# === Step 5: Multithreaded execution ===\n",
    "def process(index_row):\n",
    "    idx, row = index_row\n",
    "    path = download_image(row)\n",
    "    return idx, path\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    results = list(tqdm(executor.map(process, df.iterrows()), total=len(df), desc=\"Downloading images\"))\n",
    "    for idx, path in results:\n",
    "        df.at[idx, \"local_path\"] = path\n",
    "\n",
    "# === Step 6: Save updated Excel ===\n",
    "output_path = \"image1_metadata_with_local_paths.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9678e95-7694-495f-8c32-2e0728383eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import base64\n",
    "import mimetypes\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- LOGGING CONFIG ----------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"image_analysis.log\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "API_KEY = \"your_api_key\"\n",
    "BASE_URL = \"\"\n",
    "MODEL = \"gemini-2.5-pro\"\n",
    "\n",
    "# Directory of the current Python process (as used in original code)\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath(\"images_analysis\"))\n",
    "\n",
    "INPUT_XLSX = os.path.join(CURRENT_DIR, \"image_analysis_api.xlsx\")\n",
    "OUTPUT_XLSX = os.path.join(CURRENT_DIR, \"image_analysis_results_prompt.xlsx\")\n",
    "IMAGE_DOWNLOAD_DIR = os.path.join(CURRENT_DIR, \"/Users/dengqiuyue/Downloads/final project/images\")\n",
    "\n",
    "# Checkpointing and incremental saves\n",
    "TEMP_OUTPUT_XLSX = os.path.join(CURRENT_DIR, \"image_analysis_temp.xlsx\")\n",
    "CHECKPOINT_FILE = os.path.join(CURRENT_DIR, \"checkpoint.txt\")\n",
    "SAVE_INTERVAL = 20  # save every N processed tasks\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "REQUESTS_PER_MIN = 60\n",
    "RETRIES = 3\n",
    "BACKOFF = 2.0\n",
    "\n",
    "PROMPT_TEXT_ENGLISH = \"\"\"\n",
    "Definition: \"Core figure\" refers to the most visually prominent person in the image, determined by factors like size, central placement, lighting, or interaction focus.\n",
    "\n",
    "Part 1: Describe this image in detail.\n",
    "\n",
    "Part 2: Structured Feature List (19 fixed-category features)\n",
    "List the following 19 features using the specified categories. For numerical/categorical features, use only the provided options (do not use free text). For \"unknown\" cases, mark with \"99\".\n",
    "\n",
    "Definition: \"Core figure\" refers to the most visually prominent person in the image, determined by factors like size, central placement, lighting, or interaction focus.\n",
    "\n",
    "1. Image Subject Type\n",
    "Category: 1=Single person; 2=Multiple people; 3=No people (landscape/object)\n",
    "\n",
    "2. Core Figure Count\n",
    "Category: [Specific number, e.g., \"3\"; 99=Unknown)\n",
    "\n",
    "3. Core Figure Visual Weight\n",
    "Category: 1=Less than or equal to 1/3 of frame; 2=1/3 to 2/3 of frame (inclusive); 3=Greater than or equal to 2/3 of frame\n",
    "\n",
    "4. Source Scene Type\n",
    "Category: 1=News scene; 2=Lab/Office; 3=Public Space; 4=Private Space; 5=Abstract (illustration/diagram)\n",
    "\n",
    "5. Image Medium\n",
    "Category: 1=Photograph; 2=Illustration/Cartoon; 3=Diagram/Data Visualization; 4=Screenshot\n",
    "\n",
    "6. Core Figure Age Group\n",
    "Category: 1=Child (clear pre-pubescent features); 2=Adult (typical working-age appearance); 3=Elderly (visible gray hair/wrinkles/walking aid); 99=Uncertain (when face is obscured or unclear)\n",
    "\n",
    "7. Core Figure Gender\n",
    "Category: 1=Male; 2=Female; 3=Unclear. \n",
    "\n",
    "8. Core Figure Skin Tone\n",
    "Category: 1=White; 2=Black; 3=Beige; 4=Mixed/Unclear \n",
    "\n",
    "9. Clothing Style\n",
    "Category: 1=Formal (suit/uniform); 2=Casual; 3=Professional (lab coat/workwear); 4=Mixed\n",
    "*Note: \"Professional\" refers to clothing specific to a tech/occupational role; \"Formal\" refers to general ceremonial/official attire\n",
    "\n",
    "10. Accessories\n",
    "Category: 1=No significant accessories; 2=Tech devices (headphones/smartwatch); 3=Identity markers (badge/ID); 4=Personalized (tattoos/statement jewelry)\n",
    "\n",
    "11. Core Figure Action\n",
    "Category: 1=Static (standing/sitting); 2=Dynamic (walking/operating); 3=Interactive (speaking/demonstrating); 4=Passive (being photographed/observed)\n",
    "\n",
    "12. Core Figure Emotion\n",
    "Category: 1=Positive (smiling/confident); 2=Neutral (calm/focused); 3=Negative (serious/anxious); 4=Unclear\n",
    "\n",
    "13. Tech Device Presence\n",
    "Category: 1=No tech; 2=General (phone/laptop); 3=Specialized (AI device/instrument); 4=Media equipment (camera/microphone)\n",
    "\n",
    "14. Human-Tech Relationship\n",
    "Category: 1=Using device; 2=Surrounded by devices; 3=Independent of device; 4=No device\n",
    "\n",
    "15. Background Elements\n",
    "Category: 1=Natural (plants/sky); 2=Architectural (walls/doors); 3=Symbolic (flag/slogan); 4=Cluttered/Unfocused\n",
    "\n",
    "16. Lighting Intensity\n",
    "Category: 1=Bright; 2=Dim; 3=High Contrast; 4=Soft\n",
    "\n",
    "17. Dominant Color Tone\n",
    "Category: 1=Cool (blue/green); 2=Warm (red/yellow); 3=Neutral (black/white/gray); 4=Mixed\n",
    "\n",
    "18. Interpersonal Interaction Pattern\n",
    "Category: 1=No interaction; 2=Collaborative (working together); 3=Interview (question/answer); 4=Bystander (observing core figure)\n",
    "\n",
    "19. Core Figure Visual Subject\n",
    "Category: 1=Person; 2=Object; 3=Other\n",
    "\n",
    "Output Format:\n",
    "- Start with \"Part 1: [Caption]\"\n",
    "- Then \"Part 2: [Numbered list of features 1-19, each on a new line, e.g., '1. Image Subject Type: 2']\"\n",
    "\"\"\"\n",
    "\n",
    "# ---------------- RATE LIMITER ----------------\n",
    "class TokenBucket:\n",
    "    def __init__(self, rate_per_min):\n",
    "        self.capacity = max(1, rate_per_min)\n",
    "        self.tokens = self.capacity\n",
    "        self.refill_time = time.time()\n",
    "        self.rate_per_sec = rate_per_min / 60.0\n",
    "\n",
    "    def consume(self, row_idx=None):\n",
    "        now = time.time()\n",
    "        elapsed = now - self.refill_time\n",
    "        self.tokens = min(self.capacity, self.tokens + elapsed * self.rate_per_sec)\n",
    "        self.refill_time = now\n",
    "\n",
    "        if self.tokens < 1:\n",
    "            wait_time = (1 - self.tokens) / self.rate_per_sec\n",
    "            logger.info(f\"[Row {row_idx}] Rate limit hit, sleeping {wait_time:.2f}s (tokens: {self.tokens:.2f})\")\n",
    "            time.sleep(wait_time)\n",
    "            now = time.time()\n",
    "            elapsed = now - self.refill_time\n",
    "            self.tokens = min(self.capacity, self.tokens + elapsed * self.rate_per_sec)\n",
    "            self.refill_time = now\n",
    "\n",
    "        self.tokens -= 1\n",
    "        logger.debug(f\"[Row {row_idx}] Consumed a token, remaining: {self.tokens:.2f}\")\n",
    "\n",
    "bucket = TokenBucket(REQUESTS_PER_MIN)\n",
    "\n",
    "# ---------------- CHECKPOINT / RESUME ----------------\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load checkpoint; return a set of completed row indices.\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "                completed_indices = set(int(line.strip()) for line in f if line.strip())\n",
    "            logger.info(f\"Loaded {len(completed_indices)} completed tasks from checkpoint\")\n",
    "            return completed_indices\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to read checkpoint file: {e}\")\n",
    "    return set()\n",
    "\n",
    "def save_checkpoint(completed_indices):\n",
    "    \"\"\"Save checkpoint of completed row indices.\"\"\"\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "            for idx in sorted(completed_indices):\n",
    "                f.write(f\"{idx}\\n\")\n",
    "        logger.debug(f\"Checkpoint saved: {len(completed_indices)} tasks\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_existing_results():\n",
    "    \"\"\"Load existing temporary results file.\"\"\"\n",
    "    if os.path.exists(TEMP_OUTPUT_XLSX):\n",
    "        try:\n",
    "            df = pd.read_excel(TEMP_OUTPUT_XLSX)\n",
    "            results = df.to_dict(\"records\")\n",
    "            logger.info(f\"Loaded {len(results)} existing results from temporary file\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to read temporary results file: {e}\")\n",
    "    return []\n",
    "\n",
    "def save_results_incremental(results, force=False):\n",
    "    \"\"\"Incrementally save results to the temporary file.\"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_excel(TEMP_OUTPUT_XLSX, index=False)\n",
    "        logger.info(f\"Temporary results saved: {len(results)} records\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save temporary results: {e}\")\n",
    "        return False\n",
    "\n",
    "# ---------------- IMAGE FUNCTIONS ----------------\n",
    "def download_image(image_url, save_path, row_idx=None):\n",
    "    try:\n",
    "        logger.info(f\"[Row {row_idx}] Start downloading image: {image_url}\")\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        resp = requests.get(image_url, stream=True, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        file_size = 0\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    file_size += len(chunk)\n",
    "\n",
    "        logger.info(f\"[Row {row_idx}] Image downloaded: {save_path} ({file_size:,} bytes)\")\n",
    "        return save_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[Row {row_idx}] Image download failed {image_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _guess_mime_from_path(path):\n",
    "    mime, _ = mimetypes.guess_type(path)\n",
    "    return mime or \"image/jpeg\"\n",
    "\n",
    "def call_ai_api(image_path_local, prompt_text, row_idx=None):\n",
    "    logger.info(f\"[Row {row_idx}] Calling AI API to analyze image: {image_path_local}\")\n",
    "\n",
    "    with open(image_path_local, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    image_mime = _guess_mime_from_path(image_path_local)\n",
    "    data_url = f\"data:{image_mime};base64,{b64}\"\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Gemini-style chat completions payload\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt_text},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 20000\n",
    "    }\n",
    "\n",
    "    logger.info(f\"[Row {row_idx}] Sending API request to: {BASE_URL}/chat/completions\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(f\"{BASE_URL}/chat/completions\", headers=headers, json=payload, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        response_data = resp.json()\n",
    "        logger.info(f\"[Row {row_idx}] Raw API response (first 200 chars): {str(response_data)[:200]}...\")\n",
    "\n",
    "        # Validate response structure\n",
    "        if \"choices\" not in response_data:\n",
    "            logger.error(f\"[Row {row_idx}] Missing 'choices' in response: {response_data}\")\n",
    "            raise Exception(\"Bad API response: missing 'choices'\")\n",
    "\n",
    "        if len(response_data[\"choices\"]) == 0:\n",
    "            logger.error(f\"[Row {row_idx}] Empty 'choices' array: {response_data}\")\n",
    "            raise Exception(\"Bad API response: empty 'choices'\")\n",
    "\n",
    "        choice = response_data[\"choices\"][0]\n",
    "        if \"message\" not in choice:\n",
    "            logger.error(f\"[Row {row_idx}] Missing 'message' in choice: {choice}\")\n",
    "            raise Exception(\"Bad API response: missing 'message'\")\n",
    "\n",
    "        if \"content\" not in choice[\"message\"]:\n",
    "            logger.error(f\"[Row {row_idx}] Missing 'content' in message: {choice['message']}\")\n",
    "            raise Exception(\"Bad API response: missing 'content'\")\n",
    "\n",
    "        response_content = choice[\"message\"][\"content\"]\n",
    "\n",
    "        # Detailed logging of content shape\n",
    "        if response_content is None:\n",
    "            logger.warning(f\"[Row {row_idx}] API returned content=None (empty response cause)\")\n",
    "            logger.warning(f\"[Row {row_idx}] Full choice: {choice}\")\n",
    "            response_content = \"\"\n",
    "        elif response_content == \"\":\n",
    "            logger.warning(f\"[Row {row_idx}] API returned empty string for content\")\n",
    "            logger.warning(f\"[Row {row_idx}] Full choice: {choice}\")\n",
    "            if \"finish_reason\" in choice:\n",
    "                logger.warning(f\"[Row {row_idx}] finish_reason: {choice['finish_reason']}\")\n",
    "            if \"usage\" in response_data:\n",
    "                logger.warning(f\"[Row {row_idx}] usage: {response_data['usage']}\")\n",
    "        elif not isinstance(response_content, str):\n",
    "            logger.warning(f\"[Row {row_idx}] content is not a string: {type(response_content)}, value: {response_content}\")\n",
    "            response_content = str(response_content)\n",
    "        elif response_content.strip() == \"\":\n",
    "            logger.warning(f\"[Row {row_idx}] content contains only whitespace\")\n",
    "            logger.warning(f\"[Row {row_idx}] repr(content): {repr(response_content)}\")\n",
    "\n",
    "        # Record success metrics\n",
    "        elapsed_time = time.time() - start_time\n",
    "        usage = response_data.get(\"usage\", {})\n",
    "\n",
    "        finish_reason = choice.get(\"finish_reason\", \"unknown\")\n",
    "        if finish_reason == \"length\" and len(response_content.strip()) == 0:\n",
    "            logger.warning(f\"[Row {row_idx}] finish_reason='length' with empty response; possible token limit\")\n",
    "\n",
    "        logger.info(\n",
    "            f\"[Row {row_idx}] API call OK. Elapsed: {elapsed_time:.2f}s, \"\n",
    "            f\"total_tokens: {usage.get('total_tokens', 'N/A')}, \"\n",
    "            f\"response_len: {len(response_content)} chars, \"\n",
    "            f\"finish_reason: {finish_reason}\"\n",
    "        )\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.warning(f\"[Row {row_idx}] Primary URL failed (elapsed {elapsed_time:.2f}s): {e}\")\n",
    "\n",
    "        # Fallback: try removing '/v1'\n",
    "        if \"/v1\" in BASE_URL:\n",
    "            fallback_url = BASE_URL.replace(\"/v1\", \"\")\n",
    "            logger.info(f\"[Row {row_idx}] Trying fallback URL: {fallback_url}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(f\"{fallback_url}/chat/completions\", headers=headers, json=payload, timeout=120)\n",
    "                resp.raise_for_status()\n",
    "\n",
    "                response_data = resp.json()\n",
    "                response_content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "                usage = response_data.get(\"usage\", {})\n",
    "                logger.info(\n",
    "                    f\"[Row {row_idx}] Fallback call OK. Elapsed: {elapsed_time:.2f}s, \"\n",
    "                    f\"total_tokens: {usage.get('total_tokens', 'N/A')}, \"\n",
    "                    f\"response_len: {len(response_content)} chars\"\n",
    "                )\n",
    "\n",
    "                return response_content\n",
    "\n",
    "            except Exception as fallback_e:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                logger.error(f\"[Row {row_idx}] Fallback URL failed (elapsed {elapsed_time:.2f}s): {fallback_e}\")\n",
    "                raise fallback_e\n",
    "        else:\n",
    "            logger.error(f\"[Row {row_idx}] API call failed, no fallback URL: {e}\")\n",
    "            raise e\n",
    "    except Exception as e:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.error(f\"[Row {row_idx}] API call exception (elapsed {elapsed_time:.2f}s): {e}\")\n",
    "        raise e\n",
    "\n",
    "# ---------------- ROW PROCESSING ----------------\n",
    "def analyze_one_row(row_idx, row):\n",
    "    logger.info(f\"[Row {row_idx}] ==================== START ====================\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    article_id = row.get(\"article_id\")\n",
    "    image_id = row.get(\"image_id\")\n",
    "    image_url = str(row.get(\"image_path\", \"\")).strip()\n",
    "\n",
    "    logger.info(f\"[Row {row_idx}] Task info: article_id={article_id}, image_id={image_id}\")\n",
    "\n",
    "    if not image_url:\n",
    "        logger.error(f\"[Row {row_idx}] Error: image_url is missing\")\n",
    "        return {\"row_idx\": row_idx, \"error\": \"image_url_missing\"}\n",
    "\n",
    "    base_name = os.path.basename(image_url.split(\"?\")[0]) or f\"image_{row_idx}.jpg\"\n",
    "    if image_id:\n",
    "        base_name = f\"{image_id}_{base_name}\"\n",
    "    if not base_name.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".webp\")):\n",
    "        base_name += \".jpg\"\n",
    "    local_path = os.path.join(IMAGE_DOWNLOAD_DIR, base_name)\n",
    "\n",
    "    logger.info(f\"[Row {row_idx}] Local save path: {local_path}\")\n",
    "\n",
    "    # Download if missing\n",
    "    if not os.path.exists(local_path):\n",
    "        logger.info(f\"[Row {row_idx}] File not found locally; starting download\")\n",
    "        if not download_image(image_url, local_path, row_idx):\n",
    "            logger.error(f\"[Row {row_idx}] Task failed: image download failed\")\n",
    "            return {\"row_idx\": row_idx, \"error\": \"download_failed\"}\n",
    "    else:\n",
    "        logger.info(f\"[Row {row_idx}] File already exists; skip download\")\n",
    "\n",
    "    # Retry API calls with backoff\n",
    "    last_err, response_content = \"\", \"\"\n",
    "    for attempt in range(1, RETRIES + 1):\n",
    "        try:\n",
    "            logger.info(f\"[Row {row_idx}] API attempt {attempt}/{RETRIES}\")\n",
    "            bucket.consume(row_idx)\n",
    "            response_content = call_ai_api(local_path, PROMPT_TEXT_ENGLISH, row_idx)\n",
    "            logger.info(f\"[Row {row_idx}] API call succeeded\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            logger.error(f\"[Row {row_idx}] Attempt {attempt}/{RETRIES} failed: {e}\")\n",
    "            if attempt < RETRIES:\n",
    "                wait_time = BACKOFF ** (attempt - 1)\n",
    "                logger.info(f\"[Row {row_idx}] Sleeping {wait_time:.2f}s before retry\")\n",
    "                time.sleep(wait_time)\n",
    "    else:\n",
    "        total_time = time.time() - start_time\n",
    "        logger.error(f\"[Row {row_idx}] Task failed after all retries (elapsed {total_time:.2f}s)\")\n",
    "        return {\"row_idx\": row_idx, \"error\": last_err or \"api_failed\"}\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"[Row {row_idx}] ==================== DONE ==================== (elapsed {total_time:.2f}s)\")\n",
    "\n",
    "    # Validate response content\n",
    "    if not response_content:\n",
    "        logger.warning(f\"[Row {row_idx}] Warning: API returned empty content\")\n",
    "        response_content = \"API_RESPONSE_EMPTY\"\n",
    "\n",
    "    result = {\n",
    "        \"row_idx\": row_idx,\n",
    "        \"article_id\": article_id,\n",
    "        \"image_id\": image_id,\n",
    "        \"image_path\": image_url,\n",
    "        \"response\": response_content\n",
    "    }\n",
    "\n",
    "    logger.info(f\"[Row {row_idx}] Returning result: response length={len(str(response_content))}\")\n",
    "    return result\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "def main():\n",
    "    logger.info(\"========================================\")\n",
    "    logger.info(\"Image analysis job started\")\n",
    "    logger.info(f\"Config: workers={MAX_WORKERS}, rate_limit={REQUESTS_PER_MIN}/min, retries={RETRIES}\")\n",
    "    logger.info(f\"Incremental save interval: every {SAVE_INTERVAL} tasks\")\n",
    "    logger.info(\"========================================\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read input\n",
    "    try:\n",
    "        df = pd.read_excel(INPUT_XLSX)\n",
    "        logger.info(f\"Input file loaded: {INPUT_XLSX}, total rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read input file {INPUT_XLSX}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Filter valid rows\n",
    "    original_count = len(df)\n",
    "    df = df[df[\"image_path\"].astype(str).str.len() > 0].reset_index(drop=True)\n",
    "    valid_count = len(df)\n",
    "    logger.info(f\"Valid rows after filtering: {valid_count}/{original_count}\")\n",
    "\n",
    "    if valid_count == 0:\n",
    "        logger.warning(\"No valid image rows to process\")\n",
    "        return\n",
    "\n",
    "    # Load checkpoint and existing results\n",
    "    completed_indices = load_checkpoint()\n",
    "    existing_results = load_existing_results()\n",
    "\n",
    "    # Build index -> result map for updates\n",
    "    existing_results_dict = {r.get(\"row_idx\"): r for r in existing_results if r.get(\"row_idx\") is not None}\n",
    "\n",
    "    # Determine remaining tasks\n",
    "    remaining_indices = set(range(valid_count)) - completed_indices\n",
    "    logger.info(f\"Checkpoint: {len(completed_indices)} completed, {len(remaining_indices)} remaining\")\n",
    "\n",
    "    if not remaining_indices:\n",
    "        logger.info(\"All tasks already completed; saving final results\")\n",
    "        if existing_results:\n",
    "            pd.DataFrame(existing_results).to_excel(OUTPUT_XLSX, index=False)\n",
    "            logger.info(f\"Final results saved to: {OUTPUT_XLSX}\")\n",
    "        return\n",
    "\n",
    "    # Initialize counters\n",
    "    results = existing_results.copy()\n",
    "    completed_count = len(completed_indices)\n",
    "    success_count = sum(1 for r in existing_results if not r.get(\"error\") and str(r.get(\"response\", \"\")).strip())\n",
    "\n",
    "    logger.info(f\"Continuing: {len(existing_results)} existing results, {success_count} successful\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
    "        logger.info(f\"Starting thread pool with max_workers={MAX_WORKERS}\")\n",
    "\n",
    "        futures = {}\n",
    "        for i in remaining_indices:\n",
    "            row = df.iloc[i].to_dict()\n",
    "            future = pool.submit(analyze_one_row, i, row)\n",
    "            futures[future] = i\n",
    "\n",
    "        logger.info(f\"Submitted {len(futures)} tasks\")\n",
    "\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Images\"):\n",
    "            result = fut.result()\n",
    "            row_idx = result[\"row_idx\"]\n",
    "\n",
    "            # Update results list\n",
    "            if row_idx in existing_results_dict:\n",
    "                idx = next(i for i, r in enumerate(results) if r.get(\"row_idx\") == row_idx)\n",
    "                results[idx] = result\n",
    "            else:\n",
    "                results.append(result)\n",
    "\n",
    "            # Update counters\n",
    "            completed_count += 1\n",
    "            completed_indices.add(row_idx)\n",
    "\n",
    "            # Inspect result\n",
    "            has_error = \"error\" in result and result.get(\"error\")\n",
    "            has_response = \"response\" in result and result.get(\"response\") and str(result.get(\"response\")).strip()\n",
    "\n",
    "            if has_error:\n",
    "                logger.error(f\"Task {result['row_idx']} FAILED: {result['error']} ({completed_count}/{valid_count})\")\n",
    "            elif has_response:\n",
    "                success_count += 1\n",
    "                response_preview = str(result.get(\"response\", \"\"))[:50].replace(\"\\n\", \" \")\n",
    "                logger.info(f\"Task {result['row_idx']} SUCCESS ({completed_count}/{valid_count}) - Preview: {response_preview}...\")\n",
    "            else:\n",
    "                logger.warning(f\"Task {result['row_idx']} WARNING: neither error nor response present ({completed_count}/{valid_count})\")\n",
    "                logger.warning(f\"[Row {result['row_idx']}] Full result: {result}\")\n",
    "\n",
    "            # Periodic save\n",
    "            should_save = (\n",
    "                (completed_count % SAVE_INTERVAL == 0) or\n",
    "                (completed_count == valid_count) or\n",
    "                (len(remaining_indices) - (completed_count - len(existing_results)) <= 0)\n",
    "            )\n",
    "\n",
    "            if should_save:\n",
    "                # Save checkpoint\n",
    "                save_checkpoint(completed_indices)\n",
    "\n",
    "                # Save temporary results\n",
    "                if save_results_incremental(results):\n",
    "                    elapsed = time.time() - start_time\n",
    "                    success_rate = (success_count / completed_count) * 100\n",
    "                    empty_count = sum(1 for r in results if not r.get(\"error\") and not str(r.get(\"response\", \"\")).strip())\n",
    "                    logger.info(f\"Progress: {completed_count}/{valid_count} done, \"\n",
    "                                f\"success_rate: {success_rate:.1f}%, empty_responses: {empty_count}, elapsed: {elapsed:.1f}s\")\n",
    "                    logger.info(f\"Progress saved ({len(results)} records)\")\n",
    "\n",
    "    # Save final results\n",
    "    try:\n",
    "        results.sort(key=lambda x: x.get(\"row_idx\", 0))\n",
    "        pd.DataFrame(results).to_excel(OUTPUT_XLSX, index=False)\n",
    "        logger.info(f\"Final results saved to: {OUTPUT_XLSX}\")\n",
    "\n",
    "        # Cleanup temporary files\n",
    "        cleanup_success = True\n",
    "        try:\n",
    "            if os.path.exists(TEMP_OUTPUT_XLSX):\n",
    "                os.remove(TEMP_OUTPUT_XLSX)\n",
    "                logger.info(\"Temporary results file removed\")\n",
    "            if os.path.exists(CHECKPOINT_FILE):\n",
    "                os.remove(CHECKPOINT_FILE)\n",
    "                logger.info(\"Checkpoint file removed\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error cleaning up temporary files: {e}\")\n",
    "            cleanup_success = False\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save final results: {e}\")\n",
    "        logger.info(f\"You can recover from the temporary file: {TEMP_OUTPUT_XLSX}\")\n",
    "        return\n",
    "\n",
    "    # Final stats\n",
    "    total_time = time.time() - start_time\n",
    "    final_success_rate = (success_count / valid_count) * 100\n",
    "\n",
    "    error_count = sum(1 for r in results if r.get(\"error\"))\n",
    "    empty_response_count = sum(1 for r in results if not r.get(\"error\") and not str(r.get(\"response\", \"\")).strip())\n",
    "    valid_response_count = sum(1 for r in results if not r.get(\"error\") and str(r.get(\"response\", \"\")).strip())\n",
    "\n",
    "    logger.info(\"========================================\")\n",
    "    logger.info(\"Final statistics:\")\n",
    "    logger.info(f\"  Total tasks: {valid_count}\")\n",
    "    logger.info(f\"  Valid responses: {valid_response_count}\")\n",
    "    logger.info(f\"  Empty responses: {empty_response_count}\")\n",
    "    logger.info(f\"  Error tasks: {error_count}\")\n",
    "    logger.info(f\"  Success rate: {final_success_rate:.2f}%\")\n",
    "    logger.info(f\"  Total time: {total_time:.1f}s\")\n",
    "    logger.info(f\"  Avg per task: {total_time / valid_count:.2f}s\")\n",
    "\n",
    "    if empty_response_count > 0:\n",
    "        logger.warning(f\"Found {empty_response_count} empty responses; please check logs to diagnose\")\n",
    "        # Show a few examples\n",
    "        empty_tasks = [r for r in results if not r.get(\"error\") and not str(r.get(\"response\", \"\")).strip()][:3]\n",
    "        for task in empty_tasks:\n",
    "            logger.warning(f\"Empty response example: Row {task['row_idx']}, result: {task}\")\n",
    "\n",
    "    logger.info(\"========================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff31ad-c543-4ce5-9931-03ee7e9c92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "import mimetypes\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= LOGGING CONFIG =================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"image_role3_classifier.log\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ================= CONFIG =================\n",
    "API_KEY = \"\"  # REQUIRED: replace with your key\n",
    "BASE_URL = \"\"                    # url\n",
    "MODEL = \"gemini-2.5-pro\"                                       # or your available model name\n",
    "\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath(\"image_role3_classifier\"))\n",
    "\n",
    "# Input & output\n",
    "INPUT_XLSX  = os.path.join(CURRENT_DIR, \"cleaned_image_data.xlsx\")   # input (must include image_path; optional Caption, etc.)\n",
    "OUTPUT_XLSX = os.path.join(CURRENT_DIR, \"role_results.xlsx\")         # final output (keep all input cols + new result cols)\n",
    "TEMP_OUTPUT_XLSX = os.path.join(CURRENT_DIR, \"role_temp.xlsx\")       # incremental save\n",
    "CHECKPOINT_FILE  = os.path.join(CURRENT_DIR, \"role_checkpoint.txt\")  # checkpoint\n",
    "\n",
    "# Directory containing already-downloaded images (as you provided)\n",
    "LOCAL_IMAGE_DIR = \"/Users/dengqiuyue/Downloads/final project/images\"\n",
    "\n",
    "# Concurrency and stability controls\n",
    "MAX_WORKERS = 8\n",
    "REQUESTS_PER_MIN = 60\n",
    "RETRIES = 3\n",
    "BACKOFF = 2.0\n",
    "SAVE_INTERVAL = 20\n",
    "\n",
    "# ================= PROMPT (3-class; image first, caption secondary) =================\n",
    "PROMPT_TEXT = \"\"\"\n",
    "You are given a news image to analyze. Your task is to classify the main human subject in the image into ONE of only three categories below.\n",
    "If no person is clearly visible, or if the visual identity is ambiguous/abstract/only objects/illustrations are shown, choose 99.\n",
    "\n",
    "Target label set (choose exactly one):\n",
    "1. Elite (e.g., scientist/researcher/engineer; CEO/entrepreneur/business leader; politician/government official)\n",
    "2. General Public (non-elite everyday people, users, workers, citizens, students, audiences, crowds)\n",
    "99. Unclear/Abstract (no person clearly visible, or identity cannot be determined, or purely abstract/object/robot/diagram)\n",
    "\n",
    "Important instructions:\n",
    "- Base your judgment primarily on the IMAGE CONTENT. If a caption is provided, use it only as a secondary hint.\n",
    "- If it's not strongly evident that the person is an elite (scientist/CEO/politician), do NOT guess \"Elite\".\n",
    "- If multiple people are present, focus on the most visually prominent or socially central figure.\n",
    "- If the image does not show a person (e.g., robots, diagrams, product shots), or it's too ambiguous, use 99.\n",
    "\n",
    "Output format (strict JSON only):\n",
    "{\n",
    "  \"role\": [1 or 2 or 99],\n",
    "  \"role_label\": \"[Elite | General Public | Unclear/Abstract]\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ================= RATE LIMITER =================\n",
    "class TokenBucket:\n",
    "    def __init__(self, rate_per_min):\n",
    "        self.capacity = max(1, rate_per_min)\n",
    "        self.tokens = self.capacity\n",
    "        self.refill_time = time.time()\n",
    "        self.rate_per_sec = rate_per_min / 60.0\n",
    "\n",
    "    def consume(self, row_idx=None):\n",
    "        now = time.time()\n",
    "        elapsed = now - self.refill_time\n",
    "        self.tokens = min(self.capacity, self.tokens + elapsed * self.rate_per_sec)\n",
    "        self.refill_time = now\n",
    "        if self.tokens < 1:\n",
    "            wait_time = (1 - self.tokens) / self.rate_per_sec\n",
    "            logger.info(f\"[Row {row_idx}] Rate limit triggered, waiting {wait_time:.2f}s (tokens={self.tokens:.2f})\")\n",
    "            time.sleep(wait_time)\n",
    "            now = time.time()\n",
    "            elapsed = now - self.refill_time\n",
    "            self.tokens = min(self.capacity, self.tokens + elapsed * self.rate_per_sec)\n",
    "            self.refill_time = now\n",
    "        self.tokens -= 1\n",
    "\n",
    "bucket = TokenBucket(REQUESTS_PER_MIN)\n",
    "\n",
    "# ================= CHECKPOINT & TEMP =================\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "                return set(int(line.strip()) for line in f if line.strip())\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to read checkpoint: {e}\")\n",
    "    return set()\n",
    "\n",
    "def save_checkpoint(done_set):\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "            for idx in sorted(done_set):\n",
    "                f.write(f\"{idx}\\n\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_existing_results():\n",
    "    if os.path.exists(TEMP_OUTPUT_XLSX):\n",
    "        try:\n",
    "            return pd.read_excel(TEMP_OUTPUT_XLSX).to_dict(\"records\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to read temporary results: {e}\")\n",
    "    return []\n",
    "\n",
    "def save_results_incremental(results):\n",
    "    pd.DataFrame(results).to_excel(TEMP_OUTPUT_XLSX, index=False)\n",
    "    logger.info(f\"Temporary results saved: {len(results)} records\")\n",
    "\n",
    "# ================= UTIL: compose local image path =================\n",
    "def guess_local_image_path(row_idx, row_dict):\n",
    "    \"\"\"\n",
    "    Try to infer the local image file path from the row:\n",
    "    1) If a 'local_path' column exists and the file exists, use it;\n",
    "    2) If 'image_local' or 'image_filename' exists, try to compose the path;\n",
    "    3) Otherwise, use the URL basename from 'image_path' as the filename;\n",
    "    4) If 'image_id' exists, also try prefixed variants.\n",
    "    \"\"\"\n",
    "    # 1) Prefer explicit local path\n",
    "    for k in [\"local_path\", \"image_local\", \"local_image_path\"]:\n",
    "        p = str(row_dict.get(k, \"\")).strip()\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "\n",
    "    # 2) Use directory + filename inference\n",
    "    filename = str(row_dict.get(\"image_filename\", \"\")).strip()\n",
    "    if filename:\n",
    "        candidate = os.path.join(LOCAL_IMAGE_DIR, filename)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "\n",
    "    # From image_path URL, take basename\n",
    "    image_url = str(row_dict.get(\"image_path\", \"\")).strip()\n",
    "    if image_url:\n",
    "        base = os.path.basename(image_url.split(\"?\")[0]) or f\"image_{row_idx}.jpg\"\n",
    "        image_id = str(row_dict.get(\"image_id\", \"\")).strip()\n",
    "        variants = []\n",
    "        # Original name\n",
    "        variants.append(base)\n",
    "        # Add .jpg if missing extension\n",
    "        if not base.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".webp\")):\n",
    "            variants.append(base + \".jpg\")\n",
    "        # With image_id prefix\n",
    "        if image_id:\n",
    "            variants.append(f\"{image_id}_{base}\")\n",
    "            if not base.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".webp\")):\n",
    "                variants.append(f\"{image_id}_{base}.jpg\")\n",
    "\n",
    "        # Try each variant in the local image dir\n",
    "        for name in variants:\n",
    "            cand = os.path.join(LOCAL_IMAGE_DIR, name)\n",
    "            if os.path.exists(cand):\n",
    "                return cand\n",
    "\n",
    "    # Not found\n",
    "    return None\n",
    "\n",
    "def encode_image_as_data_url(local_path):\n",
    "    mime, _ = mimetypes.guess_type(local_path)\n",
    "    mime = mime or \"image/jpeg\"\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return f\"data:{mime};base64,{b64}\"\n",
    "\n",
    "# ================= API CALL (image + optional caption) =================\n",
    "def call_llm_for_image_role3(local_image_path: str, caption_text: str | None, row_idx=None) -> str | None:\n",
    "    \"\"\"\n",
    "    Send a multimodal message: text prompt + image_url (data URI).\n",
    "    If your backend only supports plain text, you could include 'caption_text' in the prompt.\n",
    "    Here we use an OpenAI-compatible chat/completions payload with image_url.\n",
    "    \"\"\"\n",
    "    content = [{\"type\": \"text\", \"text\": PROMPT_TEXT}]\n",
    "    if caption_text:\n",
    "        content.append({\"type\": \"text\", \"text\": f\"(Optional hint) Caption: {caption_text}\"})\n",
    "    # Add image data URL\n",
    "    data_url = encode_image_as_data_url(local_image_path)\n",
    "    content.append({\"type\": \"image_url\", \"image_url\": {\"url\": data_url}})\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 40000\n",
    "    }\n",
    "    endpoint = f\"{BASE_URL.rstrip('/')}/chat/completions\"\n",
    "\n",
    "    for attempt in range(1, RETRIES + 1):\n",
    "        try:\n",
    "            resp = requests.post(endpoint, headers=headers, json=payload, timeout=90)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[Row {row_idx}] Attempt {attempt}/{RETRIES} failed: {e}\")\n",
    "            if attempt < RETRIES:\n",
    "                time.sleep(BACKOFF ** (attempt - 1))\n",
    "    return None\n",
    "\n",
    "# ================= ROW PROCESSING =================\n",
    "def analyze_one_row(row_idx: int, row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dict containing:\n",
    "    - All original input columns (passed through)\n",
    "    - Added: response, role3, role3_label, error, local_image_path\n",
    "    \"\"\"\n",
    "    logger.info(f\"[Row {row_idx}] ===== START =====\")\n",
    "\n",
    "    # Try to locate local image path\n",
    "    local_path = guess_local_image_path(row_idx, row)\n",
    "    # Optional caption (used as a hint if present)\n",
    "    caption = str(row.get(\"Caption\", \"\")).strip() if \"Caption\" in row else None\n",
    "\n",
    "    # Initialize result with original row\n",
    "    result = dict(row)\n",
    "    result.update({\n",
    "        \"row_idx\": row_idx,\n",
    "        \"local_image_path\": local_path if local_path else \"\",\n",
    "        \"response\": \"\",\n",
    "        \"role3\": None,\n",
    "        \"role3_label\": None,\n",
    "        \"error\": None\n",
    "    })\n",
    "\n",
    "    if not local_path or not os.path.exists(local_path):\n",
    "        logger.error(f\"[Row {row_idx}] Local image not found\")\n",
    "        result[\"error\"] = \"image_not_found\"\n",
    "        return result\n",
    "\n",
    "    # Rate limit\n",
    "    bucket.consume(row_idx)\n",
    "\n",
    "    # Call the model (image first)\n",
    "    response = call_llm_for_image_role3(local_path, caption, row_idx)\n",
    "\n",
    "    # Parse JSON\n",
    "    if response:\n",
    "        try:\n",
    "            parsed = json.loads(response)\n",
    "            role_val = parsed.get(\"role\")\n",
    "            label = parsed.get(\"role_label\")\n",
    "            # Accept only 1/2/99\n",
    "            if role_val not in [1, 2, 99]:\n",
    "                raise ValueError(f\"role must be 1/2/99, got: {role_val}\")\n",
    "            result[\"role3\"] = role_val\n",
    "            result[\"role3_label\"] = label\n",
    "            result[\"response\"] = response\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[Row {row_idx}] JSON parse failed: {e} | raw: {response[:160]}...\")\n",
    "            result[\"response\"] = response\n",
    "            result[\"error\"] = f\"json_parse_error: {e}\"\n",
    "    else:\n",
    "        result[\"error\"] = \"api_failed\"\n",
    "\n",
    "    logger.info(f\"[Row {row_idx}] DONE: role3={result['role3']}, role3_label={result['role3_label']}\")\n",
    "    return result\n",
    "\n",
    "# ================= MAIN =================\n",
    "def main():\n",
    "    logger.info(\"========================================\")\n",
    "    logger.info(\"Local image 3-class classification job started (Elite / General Public / Unclear)\")\n",
    "    logger.info(f\"Concurrency={MAX_WORKERS}, Rate limit={REQUESTS_PER_MIN}/min, Retries={RETRIES}, Save interval={SAVE_INTERVAL}\")\n",
    "    logger.info(\"========================================\")\n",
    "\n",
    "    # Read input (keep all columns)\n",
    "    try:\n",
    "        df = pd.read_excel(INPUT_XLSX)\n",
    "        logger.info(f\"Loaded input: {INPUT_XLSX}, total rows: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read input: {e}\")\n",
    "        return\n",
    "\n",
    "    total = len(df)\n",
    "\n",
    "    # Resume from checkpoint\n",
    "    done_set = load_checkpoint()\n",
    "    existing_results = load_existing_results()\n",
    "    results = existing_results.copy()\n",
    "    existing_map = {r.get(\"row_idx\"): r for r in existing_results if \"row_idx\" in r}\n",
    "\n",
    "    remaining = set(range(total)) - done_set\n",
    "    logger.info(f\"Checkpoint resume: completed {len(done_set)}, remaining {len(remaining)}\")\n",
    "\n",
    "    if not remaining:\n",
    "        logger.info(\"No remaining tasks; saving final results\")\n",
    "        pd.DataFrame(results).sort_values(\"row_idx\").to_excel(OUTPUT_XLSX, index=False)\n",
    "        logger.info(f\"Final results saved to: {OUTPUT_XLSX}\")\n",
    "        return\n",
    "\n",
    "    # Concurrent processing\n",
    "    completed_count = len(done_set)\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
    "        futures = {pool.submit(analyze_one_row, i, df.iloc[i].to_dict()): i for i in remaining}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Classifying Images\"):\n",
    "            res = fut.result()\n",
    "            ridx = res[\"row_idx\"]\n",
    "\n",
    "            if ridx in existing_map:\n",
    "                idx = next(i for i, r in enumerate(results) if r.get(\"row_idx\") == ridx)\n",
    "                results[idx] = res\n",
    "            else:\n",
    "                results.append(res)\n",
    "\n",
    "            done_set.add(ridx)\n",
    "            completed_count += 1\n",
    "\n",
    "            # Incremental save & checkpoint\n",
    "            if (completed_count % SAVE_INTERVAL == 0) or (completed_count == total):\n",
    "                save_checkpoint(done_set)\n",
    "                save_results_incremental(results)\n",
    "                logger.info(f\"Progress: {completed_count}/{total}\")\n",
    "\n",
    "    # Final save\n",
    "    results.sort(key=lambda x: x.get(\"row_idx\", 0))\n",
    "    pd.DataFrame(results).to_excel(OUTPUT_XLSX, index=False)\n",
    "    logger.info(f\"Final results saved to: {OUTPUT_XLSX}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb9b31-5b5b-4318-9f74-2fd3b1a9123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-only analysis for final_images_data1.xlsx (no age features)\n",
    "# - Uses pandas + numpy + matplotlib (+ scipy, + statsmodels if available)\n",
    "# - Pure matplotlib (no seaborn), one chart per figure, no explicit colors/styles\n",
    "# - Outputs figures & CSVs to /mnt/data/outputs_image_analysis\n",
    "# - Produces:\n",
    "#   1) Overall distributions (Gender / Skin / Role)\n",
    "#   2) Cross-tabs: GenderRole, SkinRole + chi-square + standardized residuals\n",
    "#   3) Heatmaps (row % and standardized residuals)\n",
    "#   4) Optional logistic regression: Elite (1) vs General Public (0)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# ------------------------- Paths & Setup -------------------------\n",
    "IN_XLSX = \"final_images_data1.xlsx\"   # change if needed\n",
    "OUT_DIR = \"outputs_image_analysis\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------- Load data ---------------------------\n",
    "df = pd.read_excel(IN_XLSX, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Map categorical codes to labels\n",
    "gender_map = {1: \"Male\", 2: \"Female\", 3: \"Unclear\"}\n",
    "skin_map   = {1: \"White\", 2: \"Black\", 3: \"Beige\", 4: \"Mixed/Unclear\", 99: \"Unknown\"}\n",
    "role_map   = {1: \"Elite\", 2: \"General Public\", 99: \"Unclear/Abstract\"}\n",
    "\n",
    "df[\"Gender\"] = df[\"Core Figure Gender\"].map(gender_map)\n",
    "df[\"Skin\"]   = df[\"Core Figure skin\"].map(skin_map)\n",
    "df[\"Role\"]   = df[\"role\"].map(role_map)\n",
    "\n",
    "# -------------------- Helper: save bar chart ---------------------\n",
    "def save_bar_counts(series, title, fname, rotation=0):\n",
    "    counts = series.value_counts().sort_index()\n",
    "    fig = plt.figure(figsize=(7,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(counts.index.astype(str), counts.values)  # default colors\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_xlabel(\"Category\")\n",
    "    plt.xticks(rotation=rotation)\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, fname)\n",
    "    plt.savefig(out, dpi=220)\n",
    "    plt.close(fig)\n",
    "    return out\n",
    "\n",
    "# Save counts & shares to CSV\n",
    "def counts_and_perc(series, name):\n",
    "    cnt = series.value_counts(dropna=False)\n",
    "    pct = (cnt / cnt.sum()).round(4)\n",
    "    out = pd.DataFrame({f\"{name}_count\": cnt, f\"{name}_share\": pct})\n",
    "    out_path = os.path.join(OUT_DIR, f\"overall_{name.lower()}_counts_shares.csv\")\n",
    "    out.to_csv(out_path)\n",
    "    return out_path\n",
    "\n",
    "# ------------------ 1) Overall distributions --------------------\n",
    "paths = {}\n",
    "paths[\"gender_csv\"] = counts_and_perc(df[\"Gender\"], \"Gender\")\n",
    "paths[\"skin_csv\"]   = counts_and_perc(df[\"Skin\"],   \"Skin\")\n",
    "paths[\"role_csv\"]   = counts_and_perc(df[\"Role\"],   \"Role\")\n",
    "\n",
    "paths[\"gender_bar\"] = save_bar_counts(df[\"Gender\"], \"Overall Distribution: Gender\", \"overall_gender_bar.png\")\n",
    "paths[\"skin_bar\"]   = save_bar_counts(df[\"Skin\"],   \"Overall Distribution: Skin\",   \"overall_skin_bar.png\", rotation=20)\n",
    "paths[\"role_bar\"]   = save_bar_counts(df[\"Role\"],   \"Overall Distribution: Role\",   \"overall_role_bar.png\", rotation=15)\n",
    "\n",
    "# --------------- 2) Cross-tabs & chi-square tests ---------------\n",
    "def chi2_and_exports(row_var, col_var, prefix):\n",
    "    ct = pd.crosstab(df[row_var], df[col_var])  # counts\n",
    "    chi2, p, dof, expected = chi2_contingency(ct)\n",
    "    exp = pd.DataFrame(expected, index=ct.index, columns=ct.columns)\n",
    "    resid_std = (ct - exp) / np.sqrt(exp)       # standardized residuals\n",
    "    row_pct = ct.div(ct.sum(axis=1), axis=0)    # row-normalized shares\n",
    "\n",
    "    # Save tables\n",
    "    ct_path   = os.path.join(OUT_DIR, f\"{prefix}_crosstab_counts.csv\")\n",
    "    exp_path  = os.path.join(OUT_DIR, f\"{prefix}_expected_counts.csv\")\n",
    "    res_path  = os.path.join(OUT_DIR, f\"{prefix}_std_residuals.csv\")\n",
    "    pct_path  = os.path.join(OUT_DIR, f\"{prefix}_row_percent.csv\")\n",
    "    ct.to_csv(ct_path); exp.to_csv(exp_path); resid_std.to_csv(res_path); row_pct.to_csv(pct_path)\n",
    "\n",
    "    # Save a small report\n",
    "    rep_path = os.path.join(OUT_DIR, f\"{prefix}_chi2_report.txt\")\n",
    "    with open(rep_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Chi-square test: {row_var}  {col_var}\\n\")\n",
    "        f.write(f\"chi2 = {chi2:.4f}, dof = {dof}, p = {p:.6g}\\n\\n\")\n",
    "        f.write(\"Crosstab (counts):\\n\"); f.write(ct.to_string())\n",
    "        f.write(\"\\n\\nExpected counts:\\n\"); f.write(exp.to_string())\n",
    "        f.write(\"\\n\\nStandardized residuals:\\n\"); f.write(resid_std.round(3).to_string()); f.write(\"\\n\")\n",
    "    return {\n",
    "        \"ct\": ct_path, \"exp\": exp_path, \"resid\": res_path, \"pct\": pct_path, \"report\": rep_path,\n",
    "        \"row_pct_df\": row_pct, \"resid_df\": resid_std\n",
    "    }\n",
    "\n",
    "res_gender_role = chi2_and_exports(\"Gender\", \"Role\", \"gender_role\")\n",
    "res_skin_role   = chi2_and_exports(\"Skin\",   \"Role\", \"skin_role\")\n",
    "\n",
    "# ------------------ 3) Heatmaps with matplotlib -----------------\n",
    "def save_heatmap(df_mat, title, fname, fmt=\"percent\"):\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    mat = df_mat.values\n",
    "    im = ax.imshow(mat, aspect=\"auto\")  # default colormap\n",
    "\n",
    "    # Ticks\n",
    "    ax.set_xticks(np.arange(df_mat.shape[1]))\n",
    "    ax.set_yticks(np.arange(df_mat.shape[0]))\n",
    "    ax.set_xticklabels(df_mat.columns.astype(str))\n",
    "    ax.set_yticklabels(df_mat.index.astype(str), rotation=0)\n",
    "\n",
    "    # Annotate each cell\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(mat.shape[1]):\n",
    "            val = mat[i, j]\n",
    "            txt = f\"{val*100:.1f}%\" if fmt == \"percent\" else f\"{val:.2f}\"\n",
    "            ax.text(j, i, txt, ha=\"center\", va=\"center\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, fname)\n",
    "    plt.savefig(out, dpi=220)\n",
    "    plt.close(fig)\n",
    "    return out\n",
    "\n",
    "paths[\"hm_gender_role_pct\"] = save_heatmap(res_gender_role[\"row_pct_df\"], \"Gender  Role (Row %)\", \"heatmap_gender_role_pct.png\", fmt=\"percent\")\n",
    "paths[\"hm_skin_role_pct\"]   = save_heatmap(res_skin_role[\"row_pct_df\"],   \"Skin  Role (Row %)\",   \"heatmap_skin_role_pct.png\",   fmt=\"percent\")\n",
    "paths[\"hm_gender_role_res\"] = save_heatmap(res_gender_role[\"resid_df\"],  \"Gender  Role (Std Residuals)\", \"heatmap_gender_role_resid.png\", fmt=\"resid\")\n",
    "paths[\"hm_skin_role_res\"]   = save_heatmap(res_skin_role[\"resid_df\"],    \"Skin  Role (Std Residuals)\",   \"heatmap_skin_role_resid.png\",   fmt=\"resid\")\n",
    "\n",
    "# ----------- 4) Optional logistic regression (Elite vs Public) -----------\n",
    "# Predict probability of \"Elite\" (1) vs \"General Public\" (0) using Gender & Skin\n",
    "df_lr = df[df[\"Role\"].isin([\"Elite\", \"General Public\"])].copy()\n",
    "df_lr[\"y_elite\"] = (df_lr[\"Role\"] == \"Elite\").astype(int)\n",
    "\n",
    "# One-hot encode predictors (drop first to avoid multicollinearity)\n",
    "X = pd.get_dummies(df_lr[[\"Gender\", \"Skin\"]], drop_first=True)\n",
    "y = df_lr[\"y_elite\"]\n",
    "\n",
    "logit_summary_path = os.path.join(OUT_DIR, \"logit_elite_vs_public_summary.txt\")\n",
    "coef_csv_path      = os.path.join(OUT_DIR, \"logit_coefficients_odds.csv\")\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.Logit(y, X_const, missing=\"drop\").fit(disp=False)\n",
    "    with open(logit_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model.summary().as_text())\n",
    "    params = model.params\n",
    "    conf = model.conf_int()\n",
    "    or_df = pd.DataFrame({\n",
    "        \"coef\": params,\n",
    "        \"odds_ratio\": np.exp(params),\n",
    "        \"ci_low\": np.exp(conf[0]),\n",
    "        \"ci_high\": np.exp(conf[1]),\n",
    "        \"p_value\": model.pvalues\n",
    "    })\n",
    "    or_df.to_csv(coef_csv_path)\n",
    "except Exception as e:\n",
    "    # Fallback (no p-values)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X.values, y.values)\n",
    "    coef = lr.coef_[0]\n",
    "    intercept = lr.intercept_[0]\n",
    "    or_df = pd.Series(np.exp(np.r_[intercept, coef]), index=[\"Intercept\"] + list(X.columns)).to_frame(\"odds_ratio\")\n",
    "    or_df.to_csv(coef_csv_path)\n",
    "    with open(logit_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"statsmodels not available; used sklearn LogisticRegression (no p-values)\\n\")\n",
    "        f.write(or_df.to_string())\n",
    "\n",
    "# ------------------------- Output index --------------------------\n",
    "index_md = os.path.join(OUT_DIR, \"_INDEX.txt\")\n",
    "with open(index_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Generated files (image-only analysis):\\n\\n\")\n",
    "    for k, v in paths.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "    f.write(\"\\nCross-tabs & chi2 reports:\\n\")\n",
    "    for key, res in [(\"gender_role\", res_gender_role), (\"skin_role\", res_skin_role)]:\n",
    "        f.write(f\"\\n{key}:\\n\")\n",
    "        f.write(f\" - counts: {res['ct']}\\n\")\n",
    "        f.write(f\" - expected: {res['exp']}\\n\")\n",
    "        f.write(f\" - std residuals: {res['resid']}\\n\")\n",
    "        f.write(f\" - row percent: {res['pct']}\\n\")\n",
    "        f.write(f\" - report: {res['report']}\\n\")\n",
    "    f.write(f\"\\nLogistic regression summary: {logit_summary_path}\\n\")\n",
    "    f.write(f\"Logistic regression coefficients & odds ratios: {coef_csv_path}\\n\")\n",
    "\n",
    "print(f\"Done. Outputs saved to: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2deeca1-bdd9-465e-88fb-dd926d67856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 5) Yearly trends for Gender / Skin / Role -----------\n",
    "\n",
    "def proportions_by_year(col):\n",
    "    \"\"\"Return row-normalized shares of each category by Year.\"\"\"\n",
    "    tab = pd.crosstab(df[\"Year\"], df[col], normalize=\"index\")\n",
    "    return tab\n",
    "\n",
    "def save_line_from_table(tab, title, fname):\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    for c in tab.columns:\n",
    "        ax.plot(tab.index, tab[c], marker=\"o\", label=str(c))\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Share\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, fname)\n",
    "    plt.savefig(out, dpi=220)\n",
    "    plt.close(fig)\n",
    "    return out\n",
    "\n",
    "# 1. Gender by year\n",
    "year_gender = proportions_by_year(\"Gender\")\n",
    "year_gender.to_csv(os.path.join(OUT_DIR, \"year_gender_shares.csv\"))\n",
    "save_line_from_table(year_gender, \"Gender Shares by Year\", \"year_gender_lines.png\")\n",
    "\n",
    "# 2. Skin by year\n",
    "year_skin = proportions_by_year(\"Skin\")\n",
    "year_skin.to_csv(os.path.join(OUT_DIR, \"year_skin_shares.csv\"))\n",
    "save_line_from_table(year_skin, \"Skin Shares by Year\", \"year_skin_lines.png\")\n",
    "\n",
    "# 3. Role by year\n",
    "year_role = proportions_by_year(\"Role\")\n",
    "year_role.to_csv(os.path.join(OUT_DIR, \"year_role_shares.csv\"))\n",
    "save_line_from_table(year_role, \"Role Shares by Year\", \"year_role_lines.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a49d3f-d88a-4f0e-8c1e-c6d82ee62dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
