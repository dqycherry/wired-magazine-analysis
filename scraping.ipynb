{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f924aa5-9404-4f43-bb0c-cfe9d961c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Main sitemap URL\n",
    "main_sitemap_url = \"https://www.wired.com/sitemap.xml\"\n",
    "res = requests.get(main_sitemap_url)\n",
    "soup = BeautifulSoup(res.content, \"xml\")\n",
    "\n",
    "# Extract all sitemap <loc> entries\n",
    "all_sitemaps = [loc.text for loc in soup.find_all(\"loc\") if \"sitemap.xml?year=\" in loc.text]\n",
    "\n",
    "# Filter years 2014 to 2024\n",
    "sitemaps_target = [url for url in all_sitemaps if any(str(y) in url for y in range(2014, 2025))]\n",
    "\n",
    "print(f\"Total yearly sitemap links: {len(sitemaps_target)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239fa8d-625f-43e0-9ad2-89780c16d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = []\n",
    "\n",
    "for sitemap_url in sitemaps_target:\n",
    "    try:\n",
    "        r = requests.get(sitemap_url, timeout=10)\n",
    "        xml_soup = BeautifulSoup(r.content, \"xml\")\n",
    "        urls = [loc.text for loc in xml_soup.find_all(\"loc\") if \"/story/\" in loc.text]\n",
    "        article_links.extend(urls)\n",
    "        print(f\"Extracted {len(urls)} links from {sitemap_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {sitemap_url} - {e}\")\n",
    "\n",
    "# Remove duplicates\n",
    "article_links = list(set(article_links))\n",
    "\n",
    "# Save for future use\n",
    "import pandas as pd\n",
    "df_links = pd.DataFrame(article_links, columns=[\"article_url\"])\n",
    "df_links.to_csv(\"wired_article_links_2014_2024.csv\", index=False)\n",
    "print(f\"Saved {len(article_links)} article URLs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45974d3f-95a2-4c45-9c44-a3ea9490572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai beautifulsoup4 requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595dac25-7244-4da9-ac55-9ccc20fa4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#justify images\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "# Use current working directory instead of __file__\n",
    "script_dir = os.getcwd()\n",
    "os.chdir(script_dir)  # Optional: ensures relative paths behave consistently\n",
    "\n",
    "\n",
    "# Load article URLs\n",
    "csv_file = os.path.join(script_dir, \"wired_article_links_2014_2024.xlxs\")\n",
    "if not os.path.exists(csv_file):\n",
    "    print(f\"Error: Cannot find {csv_file}\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Script directory: {script_dir}\")\n",
    "    exit(1)\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "urls = df[\"article_url\"].dropna().unique()\n",
    "print(f\"Loaded {len(urls)} unique URLs from CSV file\")\n",
    "\n",
    "# Load previous progress if exists\n",
    "progress_file = os.path.join(script_dir, \"temp_articles_with_images.xlxs\")\n",
    "final_output = os.path.join(script_dir, \"wired_articles_with_images_only.xlxs\")\n",
    "checked_set = set()\n",
    "\n",
    "# Check already processed articles\n",
    "if os.path.exists(final_output):\n",
    "    df_existing = pd.read_csv(final_output)\n",
    "    checked_set = set(df_existing[\"article_url\"])\n",
    "    print(f\"Found existing results: {len(checked_set)} URLs already processed.\")\n",
    "elif os.path.exists(progress_file):\n",
    "    df_prev = pd.read_csv(progress_file)\n",
    "    checked_set = set(df_prev[\"article_url\"])\n",
    "    print(f\"Resuming from previous session: {len(checked_set)} URLs already checked.\")\n",
    "\n",
    "# Function to check if an article contains an image\n",
    "def article_contains_image(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        article = soup.find(\"article\")\n",
    "        if not article:\n",
    "            return url, False\n",
    "        # Only check for the first image, return True immediately if found\n",
    "        image = article.find(\"img\")\n",
    "        # Explicitly close the response and release memory\n",
    "        response.close()\n",
    "        del soup, response\n",
    "        gc.collect()  # Force garbage collection\n",
    "        return url, image is not None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return url, False\n",
    "\n",
    "# Multithreading parameters\n",
    "MAX_WORKERS = 20  # Number of concurrent threads\n",
    "BATCH_SIZE = 100  # Number of URLs per batch\n",
    "batch_results = []\n",
    "results_lock = threading.Lock()  # Thread lock to protect shared resources\n",
    "\n",
    "def process_batch(batch_urls, batch_start_index):\n",
    "    \"\"\"Function to process a batch of URLs\"\"\"\n",
    "    batch_results_local = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks, with index tracking\n",
    "        future_to_info = {}\n",
    "        for idx, url in enumerate(batch_urls):\n",
    "            global_idx = batch_start_index + idx\n",
    "            future = executor.submit(article_contains_image, url)\n",
    "            future_to_info[future] = (url, global_idx)\n",
    "        \n",
    "        # Collect results\n",
    "        for future in as_completed(future_to_info):\n",
    "            url, global_idx = future_to_info[future]\n",
    "            _, has_image = future.result()\n",
    "            if has_image:\n",
    "                batch_results_local.append({\"article_url\": url})\n",
    "            print(f\"[{global_idx+1}/{len(urls_to_process)}] Processed: {url} - Has image: {has_image}\")\n",
    "    \n",
    "    return batch_results_local\n",
    "\n",
    "# Filter URLs that haven't been processed yet\n",
    "urls_to_process = [url for url in urls if url not in checked_set]\n",
    "print(f\"URLs to process: {len(urls_to_process)}\")\n",
    "\n",
    "# Process URLs in batches\n",
    "for i in range(0, len(urls_to_process), BATCH_SIZE):\n",
    "    batch_urls = urls_to_process[i:i + BATCH_SIZE]\n",
    "    batch_num = i // BATCH_SIZE + 1\n",
    "    total_batches = (len(urls_to_process) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    print(f\"\\nProcessing batch {batch_num}/{total_batches} ({len(batch_urls)} URLs)\")\n",
    "    \n",
    "    # Process current batch with multithreading\n",
    "    batch_start_index = i\n",
    "    batch_results_local = process_batch(batch_urls, batch_start_index)\n",
    "    \n",
    "    # Save batch results\n",
    "    if batch_results_local:\n",
    "        with results_lock:\n",
    "            # Save to temporary progress file\n",
    "            mode = 'a' if os.path.exists(progress_file) else 'w'\n",
    "            header = not os.path.exists(progress_file)\n",
    "            \n",
    "            df_batch = pd.DataFrame(batch_results_local)\n",
    "            df_batch.to_csv(progress_file, mode=mode, header=header, index=False)\n",
    "            \n",
    "            print(f\"Saved {len(batch_results_local)} results from batch {batch_num}\")\n",
    "            \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    \n",
    "    # Short sleep between batches to avoid overloading\n",
    "    time.sleep(1)\n",
    "\n",
    "# After processing, rename the temp file to final output\n",
    "if os.path.exists(progress_file):\n",
    "    if os.path.exists(final_output):\n",
    "        os.remove(final_output)  # Delete old final file\n",
    "    os.rename(progress_file, final_output)\n",
    "    print(f\"Processing completed. Progress file renamed to {final_output}\")\n",
    "else:\n",
    "    print(\"No results found.\")\n",
    "\n",
    "print(f\"Finished. Results saved to {final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35a3f4-864c-4f24-bcb2-98799c697d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load structured results\n",
    "df = pd.read_csv(\"wired_ai_cs_articles_full_with_images.xlxs\")\n",
    "\n",
    "# Filter: at least 1 image, valid text, and valid date\n",
    "df_filtered = df[\n",
    "    (df[\"num_images\"] > 0) &\n",
    "    (df[\"text\"].notnull()) & (df[\"text\"].str.strip() != \"\") &\n",
    "    (df[\"date\"].notnull()) & (df[\"date\"].str.strip() != \"\")\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Save filtered results\n",
    "df_filtered.to_csv(\"wired_ai_cs_articles_with_images_only.xlxs\", index=False)\n",
    "print(f\"Saved {len(df_filtered)} articles with images to wired_ai_cs_articles_with_images_only.xlxs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89c714-5a1b-4cab-8da6-4eb6b6e15b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use YOLO\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# === Step 1: Load Excel ===\n",
    "input_path = \"/Users/dengqiuyue/Downloads/final project/scraping/final_merged_metadata_with_labels.xlsx\"\n",
    "df = pd.read_excel(input_path)\n",
    "df[\"local_path\"] = \"\"\n",
    "\n",
    "# === Step 2: Prepare base folder ===\n",
    "base_folder = \"final project code/image1\"\n",
    "os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "# === Step 3: Clean article_url to valid folder path\n",
    "def safe_folder_name(article_url):\n",
    "    return (\n",
    "        str(article_url)\n",
    "        .replace(\"https://\", \"https___\")\n",
    "        .replace(\"http://\", \"http___\")\n",
    "        .replace(\"/\", \"_\")\n",
    "        .replace(\":\", \"_\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "# === Step 4: Download function\n",
    "def download_image(row):\n",
    "    try:\n",
    "        url = str(row[\"image_path\"]).strip()\n",
    "        folder_name = safe_folder_name(row[\"article_id\"])\n",
    "        image_id = str(row[\"image_id\"]).replace(\"/\", \"_\")\n",
    "\n",
    "        folder = os.path.join(base_folder, folder_name)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        save_path = os.path.join(folder, f\"{image_id}.jpg\")\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "            r = requests.get(url, headers=headers, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "        return save_path\n",
    "    except Exception as e:\n",
    "        return \"download_error\"\n",
    "\n",
    "# === Step 5: Multithreaded execution ===\n",
    "def process(index_row):\n",
    "    idx, row = index_row\n",
    "    path = download_image(row)\n",
    "    return idx, path\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    results = list(tqdm(executor.map(process, df.iterrows()), total=len(df), desc=\"Downloading images\"))\n",
    "    for idx, path in results:\n",
    "        df.at[idx, \"local_path\"] = path\n",
    "\n",
    "# === Step 6: Save updated Excel ===\n",
    "output_path = \"image1_metadata_with_local_paths.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3a0e6-9bbe-4b32-9622-63bad166f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ai related\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "base_url = \"\"\n",
    "api_key = \"your api key\"\n",
    "model = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "input_file = \"wired_articles_with_images_only.xlxs\"\n",
    "output_file = \"ai2_classification_qwen1.xlxs\"\n",
    "temp_file = \"temp_qwen_batch.xlxs\"\n",
    "fail_log = \"fail_log_qwen.xlxs\"\n",
    "batch_size = 10\n",
    "max_workers = 2\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load input\n",
    "df = pd.read_csv(input_file)\n",
    "if \"article_url\" not in df.columns:\n",
    "    raise ValueError(\"Input file must contain 'article_url' column\")\n",
    "urls = df[\"article_url\"].dropna().unique().tolist()\n",
    "\n",
    "# Load existing progress\n",
    "done_urls = set()\n",
    "results = []\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    try:\n",
    "        df_done = pd.read_csv(output_file)\n",
    "        if \"article_url\" in df_done.columns:\n",
    "            done_urls.update(df_done[\"article_url\"])\n",
    "            results.extend(df_done.to_dict(\"records\"))\n",
    "            logging.info(f\"Loaded {len(df_done)} from output file.\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to load output_file: {e}\")\n",
    "\n",
    "if os.path.exists(temp_file):\n",
    "    try:\n",
    "        df_temp = pd.read_csv(temp_file)\n",
    "        if \"article_url\" in df_temp.columns:\n",
    "            done_urls.update(df_temp[\"article_url\"])\n",
    "            results.extend(df_temp.to_dict(\"records\"))\n",
    "            logging.info(f\"Loaded {len(df_temp)} from temp file.\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to load temp_file: {e}\")\n",
    "\n",
    "# Load previous failures\n",
    "failure_records = []\n",
    "if os.path.exists(fail_log):\n",
    "    try:\n",
    "        df_fail = pd.read_csv(fail_log)\n",
    "        failure_records = df_fail.to_dict(\"records\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Extract text\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        article = soup.find(\"article\") or soup.find(\"main\") or soup.find(\"body\")\n",
    "        if not article:\n",
    "            return \"\"\n",
    "        paragraphs = article.find_all(\"p\")\n",
    "        return \"\\n\".join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 10)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to extract text from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# AI classification\n",
    "def is_ai_related(text):\n",
    "    if not text or len(text.strip()) < 50:\n",
    "        return \"no_text\"\n",
    "\n",
    "    prompt = f\"\"\"Only answer in this exact format:\n",
    "\n",
    "JUDGMENT: [true/false]\n",
    "\n",
    "Answer \"true\" if the article is in any way related to artificial intelligence (AI), including:\n",
    "- AI technologies (e.g., machine learning, neural networks, natural language processing),\n",
    "- AI-enabled applications (e.g., personalized systems, autonomous vehicles),\n",
    "- AI companies, people, ethics, research, or market trends.\n",
    "\n",
    "Answer \"false\" if the article is not related to AI.\n",
    "\n",
    "Article text:\n",
    "{text}\"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = json.dumps({\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = requests.post(base_url + \"/chat/completions\", headers=headers, data=payload, timeout=30)\n",
    "        if \"application/json\" not in response.headers.get(\"Content-Type\", \"\"):\n",
    "            raise ValueError(\"Non-JSON response\")\n",
    "\n",
    "        data = response.json()\n",
    "        if \"choices\" not in data:\n",
    "            logging.error(f\"Unexpected response: {json.dumps(data, indent=2)}\")\n",
    "            return \"api_error\"\n",
    "\n",
    "        full_response = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        for line in full_response.split('\\n'):\n",
    "            if line.lower().startswith(\"judgment:\"):\n",
    "                judgment = line.split(\":\", 1)[1].strip().lower()\n",
    "                return \"true\" if judgment.startswith(\"true\") else \"false\"\n",
    "\n",
    "        return \"true\" if full_response.lower().startswith(\"true\") else \"false\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"API ERROR for text length {len(text)}: {e}\")\n",
    "        try:\n",
    "            logging.error(f\"Response text: {response.text[:300]}\")\n",
    "        except:\n",
    "            pass\n",
    "        return \"api_error\"\n",
    "\n",
    "# URL processing\n",
    "def process_url(url):\n",
    "    if url in done_urls:\n",
    "        return None\n",
    "\n",
    "    text = extract_text(url)\n",
    "    if not text:\n",
    "        failure_records.append({\"article_url\": url, \"reason\": \"no_text\"})\n",
    "        return None\n",
    "\n",
    "    label = is_ai_related(text)\n",
    "    if label == \"api_error\":\n",
    "        failure_records.append({\"article_url\": url, \"reason\": \"api_error\"})\n",
    "        return None\n",
    "    elif label == \"no_text\":\n",
    "        failure_records.append({\"article_url\": url, \"reason\": \"too_short\"})\n",
    "        return None\n",
    "    else:\n",
    "        return {\"article_url\": url, \"is_ai_related\": label, \"text_length\": len(text)}\n",
    "\n",
    "# Multithreaded processing\n",
    "new_results = []\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(process_url, url): url for url in urls if url not in done_urls}\n",
    "    for i, future in enumerate(tqdm(as_completed(futures), total=len(futures), desc=\"Processing\")):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                logging.info(f\"[{i+1}] {result['is_ai_related'].upper()} <- {result['article_url']}\")\n",
    "                new_results.append(result)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Thread error: {e}\")\n",
    "\n",
    "        if len(new_results) >= batch_size:\n",
    "            pd.DataFrame(new_results).to_csv(temp_file, mode='a', header=not os.path.exists(temp_file), index=False)\n",
    "            results.extend(new_results)\n",
    "            new_results = []\n",
    "            time.sleep(1)\n",
    "\n",
    "# Final flush\n",
    "if new_results:\n",
    "    pd.DataFrame(new_results).to_csv(temp_file, mode='a', header=not os.path.exists(temp_file), index=False)\n",
    "    results.extend(new_results)\n",
    "\n",
    "# Final save\n",
    "pd.DataFrame(results).to_csv(output_file, index=False)\n",
    "if failure_records:\n",
    "    pd.DataFrame(failure_records).to_csv(fail_log, index=False)\n",
    "\n",
    "logging.info(\"All processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
